{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module Import\n",
    "___\n",
    "This cell imports several essential libraries and modules commonly used in data analysis, machine learning, and visualization tasks. Here's a breakdown of its components:\n",
    "\n",
    "1. **Core Libraries**: \n",
    "   - `pandas` (imported as `pd`) is a powerful library for data manipulation and analysis, offering data structures like DataFrames for handling tabular data.\n",
    "\n",
    "2. **Visualization**:\n",
    "   - `matplotlib.pyplot` (imported as `plt`) is a widely used library for creating static, interactive, and animated visualizations in Python.\n",
    "\n",
    "3. **Date and Time Handling**:\n",
    "   - `datetime` is a standard Python module for working with dates and times.\n",
    "\n",
    "4. **Warnings Management**:\n",
    "   - The `warnings` module is used to control the behavior of warning messages. The `simplefilter` function is called to suppress `UserWarning` messages by setting the action to `\"ignore\"`. This is useful in scenarios where non-critical warnings might clutter the output.\n",
    "\n",
    "5. **Machine Learning Libraries**:\n",
    "   - `xgboost` is a popular library for gradient boosting, often used in machine learning tasks for its efficiency and performance.\n",
    "   - `sklearn.metrics` provides various metrics for evaluating machine learning models. Specific metrics imported here include:\n",
    "     - `accuracy_score` for classification performance.\n",
    "     - ``roc_auc_score` for evaluating classification models using Receiver Operating Characteristic (ROC) curves.\n",
    "   - `sklearn.model_selection` includes tools like `train_test_split` for splitting datasets into training and testing subsets.\n",
    "\n",
    "6. **Type Ignoring**:\n",
    "   - The `# type: ignore` comments are used to suppress type-checking warnings from tools like `mypy`. This is often done when type annotations are not provided or when the imported modules might cause false positives in type-checking.\n",
    "\n",
    "Overall, this setup prepares the environment for a machine learning workflow, including data preprocessing, model training, evaluation, and visualization, while suppressing unnecessary warnings to maintain a clean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "G8bltmBFJYz0",
    "nbgrader": {
     "checksum": "4a9ea567916fa9abc166ea3788f5341a",
     "grade": false,
     "grade_id": "cell-6fe9a77442bf5576",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "import pandas                  as     pd                              # type: ignore\n",
    "from   datetime                import datetime\n",
    "import warnings \n",
    "import xgboost                                                         # type: ignore\n",
    "from   sklearn.metrics         import accuracy_score                   # type: ignore\n",
    "from   sklearn.metrics         import roc_auc_score                    # type: ignore\n",
    "from   sklearn.model_selection import train_test_split                 # type: ignore\n",
    "from   sklearn.preprocessing   import LabelEncoder                     # type: ignore\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Import\n",
    "Using the Diabetes dataset.\n",
    "___\n",
    "This line of code uses the `read_csv` function from the `pandas` library to load data from a CSV file named `\"diabetes.csv\"` into a DataFrame object, which is assigned to the variable `df`. A DataFrame is a two-dimensional, tabular data structure in pandas that is widely used for data manipulation and analysis.\n",
    "\n",
    "The `read_csv` function is highly versatile and can handle various parameters to customize how the CSV file is read. In this case, no additional arguments are provided, so the function uses its default settings. By default:\n",
    "- The first row of the CSV file is assumed to contain column headers, which are used as the column names in the DataFrame.\n",
    "- The data is read as-is, with no specific parsing or transformations applied.\n",
    "\n",
    "This operation is typically the first step in a data analysis or machine learning workflow, as it loads the raw data into memory for further processing. The `\"diabetes.csv\"` file likely contains information relevant to the diabetes dataset, such as patient attributes and outcomes, which will be used in subsequent steps for analysis or model training. \n",
    "\n",
    "If the file is not located in the same directory as the script or notebook, a `FileNotFoundError` will occur, and the file path will need to be adjusted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 20245,
     "status": "ok",
     "timestamp": 1675033854476,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "WzT0FmjFNjva",
    "nbgrader": {
     "checksum": "12c90a7c96c33d0b7abf260c30c3d7f2",
     "grade": false,
     "grade_id": "cell-84104bc2ef0c3d62",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "722baafd-b267-474f-f52d-9bfe56211304"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"files/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Frame Information\n",
    "___\n",
    "The `df.info()` method is a built-in function in pandas that provides a concise summary of a DataFrame. When called on the `df` DataFrame, it outputs key information about the dataset, which is useful for understanding its structure and contents. This method is particularly helpful during the initial stages of data exploration.\n",
    "\n",
    "The summary includes:\n",
    "1. **Index and Column Details**: It displays the range of the DataFrame's index and lists all column names along with their data types (e.g., integers, floats, objects).\n",
    "2. **Non-Null Counts**: For each column, it shows the number of non-null (non-missing) entries, helping identify columns with missing data.\n",
    "3. **Data Types**: It specifies the data type of each column, such as `int64`, `float64`, or `object` (used for strings or mixed data types).\n",
    "4. **Memory Usage**: It provides an estimate of the memory consumed by the DataFrame, which is useful for optimizing performance when working with large datasets.\n",
    "\n",
    "The `# type: ignore` comment is used to suppress type-checking warnings from tools like `mypy`. This might be necessary if the type-checker raises false positives or if type annotations are not fully compatible with the code.\n",
    "\n",
    "In this context, calling `df.info()` on the diabetes dataset allows the user to quickly assess the dataset's structure, identify missing values, and verify that the data types align with the intended analysis or machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1675033855223,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "PlmFmwdX8Qfl",
    "nbgrader": {
     "checksum": "dca04a94cf130fa833c1626d3d4b93e5",
     "grade": false,
     "grade_id": "cell-b11d7862b43eb1f9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "95d87cff-ce4e-4327-d08f-ebe1ff255c98"
   },
   "outputs": [],
   "source": [
    "df.info() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Data Frame Information\n",
    "___\n",
    "The `df.head(6)` method is a pandas function that retrieves the first `n` rows of a DataFrame, where `n` is specified as an argument. In this case, the argument `6` instructs the method to return the first six rows of the `df` DataFrame. If no argument is provided, the default value is `5`, meaning it would return the first five rows.\n",
    "\n",
    "This method is commonly used during the initial stages of data exploration to quickly inspect the structure and contents of a dataset. It allows the user to view a sample of the data, including column names, data types, and representative values, without loading the entire dataset into the output.\n",
    "\n",
    "The `# type: ignore` comment is included to suppress type-checking warnings from tools like `mypy`. This might be necessary if the type-checker raises false positives or if type annotations are not fully compatible with the code.\n",
    "\n",
    "In this context, calling `df.head(6)` on the diabetes dataset provides a quick preview of the first six rows, helping the user verify that the data has been loaded correctly and gain an initial understanding of its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1675033855225,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "dA0BhU6gJ5cj",
    "nbgrader": {
     "checksum": "bf006ae4b4e548a575a015f6591ec7d2",
     "grade": false,
     "grade_id": "cell-fd3a1f7c75c6a5e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "dbe5b385-1a53-4c5f-c888-121caa668c48"
   },
   "outputs": [],
   "source": [
    "df.head(6) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Percentage Distribution of Outcome\n",
    "___\n",
    "This cell calculates and displays the percentage distribution of the values in the `Outcome` column of the `df` DataFrame. Here's a detailed explanation of each step:\n",
    "\n",
    "1. **Counting Unique Values**:\n",
    "   - The `df['Outcome'].value_counts()` method counts the occurrences of each unique value in the `Outcome` column. The result is a pandas Series where the index represents the unique values, and the corresponding values represent their counts. For example, if the `Outcome` column contains binary values (e.g., `0` for no diabetes and `1` for diabetes), this method will return the count of `0`s and `1`s.\n",
    "\n",
    "2. **Calculating Percentages**:\n",
    "   - The `counts / counts.sum() * 100` operation converts the raw counts into percentages. The `counts.sum()` method calculates the total number of entries in the `Outcome` column. Dividing each count by this total and multiplying by 100 gives the percentage of each unique value relative to the total.\n",
    "\n",
    "3. **Displaying the Results**:\n",
    "   - The `print(outcomes)` statement outputs the resulting percentages to the console. The `outcomes` variable is a pandas Series where the index represents the unique values in the `Outcome` column, and the values represent their respective percentages.\n",
    "\n",
    "This code is particularly useful for understanding the class distribution in a classification problem. For example, in the context of a diabetes dataset, it helps determine whether the dataset is balanced (i.e., similar percentages for each class) or imbalanced (i.e., one class significantly outnumbers the other). This information is crucial for selecting appropriate machine learning models and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1675033855226,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "5KgU2bXyOFF2",
    "nbgrader": {
     "checksum": "3e71e8baa407436c72e6fae8f17c006a",
     "grade": false,
     "grade_id": "cell-d948ad77bce8b96d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "8fa68d7b-4bcc-4593-ccfc-dd4c24937e6f"
   },
   "outputs": [],
   "source": [
    "counts = df['Outcome'].value_counts()\n",
    "outcomes = counts / counts.sum() * 100\n",
    "print(outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Test Datasets\n",
    "\n",
    "Let's split the data 70/30 into a training set (which we will use to build models) and a test set (on which we will evaluate any model we build).\n",
    "___\n",
    "This cell separates the features (independent variables) and the target (dependent variable) from the `df` DataFrame, which is a common preprocessing step in machine learning workflows.\n",
    "\n",
    "1. **Dropping the Target Column**:\n",
    "   - The `df.drop(['Outcome'], axis=1)` method creates a new DataFrame by removing the `Outcome` column from `df`. The `axis=1` argument specifies that the operation should be performed on columns (as opposed to rows, which would require `axis=0`). The resulting DataFrame, assigned to the variable `X`, contains all the features (independent variables) that will be used to train the machine learning model.\n",
    "\n",
    "2. **Extracting the Target Column**:\n",
    "   - The `df['Outcome']` expression selects the `Outcome` column from the `df` DataFrame and assigns it to the variable `y`. This column represents the target variable, which the model will learn to predict. In the context of the diabetes dataset, the `Outcome` column likely contains binary values (e.g., `0` for no diabetes and `1` for diabetes).\n",
    "\n",
    "By separating the features (`X`) and the target (`y`), this code prepares the data for further steps such as splitting into training and testing sets, feature scaling, and model training. This separation is essential because the target variable should not be included in the feature set during model training to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "981e35bca77d256b6de0eda70478c83e",
     "grade": true,
     "grade_id": "cell-252667f943d14ef1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Outcome'], axis=1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell uses the `LabelEncoder` class from `sklearn.preprocessing` to encode the target variable `y` into a numerical format. This is a common preprocessing step in machine learning workflows, especially when the target variable contains categorical labels.\n",
    "\n",
    "1. **Creating an Instance of `LabelEncoder`**:\n",
    "   - The `LabelEncoder` object is instantiated and assigned to the variable `label_encoder`. This object will be used to fit the target variable and transform its labels into numerical values.\n",
    "\n",
    "2. **Fitting the Encoder**:\n",
    "   - The `fit` method is called on the `label_encoder` object with `y` as the input. This method analyzes the unique values in `y` and stores them in the `classes_` attribute of the encoder. For example, if `y` contains labels like `[\"yes\", \"no\", \"yes\"]`, the encoder will identify the unique classes as `[\"no\", \"yes\"]`.\n",
    "\n",
    "3. **Transforming the Labels**:\n",
    "   - The `transform` method is then called on `label_encoder` with `y` as the input. This method maps each label in `y` to its corresponding numerical value based on the order of the unique classes identified during the `fit` step. For instance, if the classes are `[\"no\", \"yes\"]`, the label `no` will be encoded as `0` and `yes` as `1`. The transformed labels are then reassigned to the variable `y`.\n",
    "\n",
    "The `# type: ignore` comments are included to suppress type-checking warnings from tools like `mypy`. This might be necessary if the type-checker raises false positives or if type annotations are not fully compatible with the code.\n",
    "\n",
    "In summary, this code converts the categorical target variable `y` into a numerical format, which is required by most machine learning algorithms. This transformation ensures that the model can process the target variable effectively during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c010e89b7f13e857bc4c5a4fce084030",
     "grade": false,
     "grade_id": "cell-23567eb2a531a7c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y)       # type: ignore\n",
    "y             = label_encoder.transform(y) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs a crucial step in preparing data for machine learning by splitting the dataset into training and testing subsets. Here's a detailed explanation of each line:\n",
    "\n",
    "1. **Splitting the Dataset**:\n",
    "   - The `train_test_split` function from `sklearn.model_selection` is used to split the features (`X`) and target (`y`) into training and testing sets. \n",
    "   - The `test_size=0.3` parameter specifies that 30% of the data will be allocated to the test set, while the remaining 70% will be used for training.\n",
    "   - The `random_state=7` parameter ensures reproducibility by controlling the random shuffling of the data. Using the same `random_state` value across runs guarantees that the split will be identical each time.\n",
    "\n",
    "   The function returns four outputs:\n",
    "   - `X_train`: The training subset of the features.\n",
    "   - `X_test`: The testing subset of the features.\n",
    "   - `y_train`: The training subset of the target variable.\n",
    "   - `y_test`: The testing subset of the target variable.\n",
    "\n",
    "2. **Evaluation Set**:\n",
    "   - The `eval_set` variable is defined as a list containing a tuple of `X_test` and `y_test`. This is often used in machine learning workflows to evaluate the model's performance on unseen data during training or validation.\n",
    "\n",
    "3. **Printing Shapes**:\n",
    "   - The `print(X_train.shape, X_test.shape)` statement outputs the dimensions of the training and testing feature sets. The `.shape` attribute of a pandas DataFrame or NumPy array returns a tuple representing the number of rows (samples) and columns (features). This helps verify that the data has been split correctly and that the proportions align with the specified `test_size`.\n",
    "\n",
    "In summary, this code prepares the data for model training and evaluation by creating separate training and testing subsets, ensuring reproducibility, and providing a quick check of the resulting dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a822f0d56bae6b0fb66f9473636ae87a",
     "grade": false,
     "grade_id": "cell-0bb555ad393382e7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "eval_set = [(X_test, y_test)]\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an XGBoost classifier on a dataset, evaluates its performance, and outputs key metrics along with the training duration. Here's a detailed explanation:\n",
    "\n",
    "1. **Initialization and Timer Start**:\n",
    "   - The `print` statement indicates the start of the training process for the XGBoost classifier.\n",
    "   - The `datetime.now()` function records the current time in the variable `st`, which will later be used to calculate the total training time.\n",
    "\n",
    "2. **XGBoost Classifier Setup**:\n",
    "   - An instance of `xgboost.XGBClassifier` is created and assigned to the variable `clf`. The classifier is configured with the following hyperparameters:\n",
    "     - `objective=\"binary:logistic\"`: Specifies that the task is binary classification, and the model will output probabilities.\n",
    "     - `learning_rate=0.05`: Controls the step size during optimization, balancing convergence speed and accuracy.\n",
    "     - `seed=9616`: Ensures reproducibility by setting a random seed.\n",
    "     - `max_depth=20`: Limits the maximum depth of each decision tree, controlling model complexity and overfitting.\n",
    "     - `gamma=10`: Adds a regularization term to control tree splitting, reducing overfitting.\n",
    "     - `n_estimators=500`: Specifies the number of boosting rounds (trees) to build.\n",
    "\n",
    "3. **Evaluation Set**:\n",
    "   - The `eval_set` variable is defined as a list containing a tuple of `X_test` and `y_test`. This set will be used to monitor the model's performance on unseen data during training.\n",
    "\n",
    "4. **Model Training**:\n",
    "   - The `clf.fit` method trains the XGBoost classifier using the training data (`X_train` and `y_train`). The `eval_set` is passed to evaluate the model's performance during training. The `verbose=False` parameter suppresses detailed training output.\n",
    "\n",
    "5. **Predictions**:\n",
    "   - The `clf.predict` method generates predictions (`y_pred`) for the test dataset (`X_test`).\n",
    "   - The `clf.predict_proba` method calculates the predicted probabilities for each class. The `[1]` index extracts the probabilities for the positive class (e.g., diabetes present).\n",
    "\n",
    "6. **Performance Metrics**:\n",
    "   - The `accuracy_score` function computes the accuracy of the predictions (`y_pred`) compared to the true labels (`y_test`).\n",
    "   - The `roc_auc_score` function calculates the Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) score using the predicted probabilities (`y_pred_proba`). This metric evaluates the model's ability to distinguish between classes.\n",
    "\n",
    "7. **Training Time and Results**:\n",
    "   - The total training time is calculated as the difference between the current time (`datetime.now()`) and the start time (`st`).\n",
    "   - The `print` statements display the training time, accuracy, and ROC-AUC score, formatted as percentages with high precision.\n",
    "\n",
    "In summary, this code trains an XGBoost classifier, evaluates its performance on the test dataset, and outputs key metrics (accuracy and ROC-AUC) along with the training duration. These steps are essential for assessing the model's effectiveness and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fbc4279db12567abe1e1a8ab37f8f267",
     "grade": false,
     "grade_id": "cell-94253767767a3a32",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "print('Initializing xgboost.sklearn.XGBClassifier and starting training...')\n",
    "st  = datetime.now()\n",
    "clf = xgboost.XGBClassifier(\n",
    "    objective=\"binary:logistic\", \n",
    "    learning_rate=0.05, \n",
    "    seed=9616, \n",
    "    max_depth=20, \n",
    "    gamma=10, \n",
    "    n_estimators=500\n",
    ")\n",
    "eval_set = [(X_test, y_test)]\n",
    "clf.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "# Predictions\n",
    "# Accuracy and ROC-AUC\n",
    "y_pred       = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "accuracy     = accuracy_score(y_test, y_pred)\n",
    "roc_auc      = roc_auc_score(y_test, y_pred_proba)\n",
    "print('Training time:', datetime.now() - st)\n",
    "print(\"Accuracy     : %.10f%%\" % (accuracy * 100.0))\n",
    "print(\"ROC-AUC      : %.10f%%\" % (roc_auc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7f772c78fe794d04cdf21d80abb6bf1",
     "grade": false,
     "grade_id": "cell-d77bf3809e323ef3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### View the results of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xgboost.plot_importance(clf)` function generates a bar chart to visualize the importance of features used by the trained XGBoost model (`clf`). This visualization helps identify which features contribute the most to the model's predictions, providing insights into the dataset and the model's decision-making process.\n",
    "\n",
    "1. **Feature Importance**:\n",
    "   - The function calculates feature importance based on the fitted trees in the XGBoost model. By default, the importance is measured using the \"weight\" metric, which counts the number of times each feature is used in tree splits. Other metrics, such as \"gain\" (average improvement in accuracy from splits) or \"cover\" (average number of samples affected by splits), can also be used.\n",
    "\n",
    "2. **Visualization**:\n",
    "   - The function creates a horizontal bar chart where:\n",
    "     - The y-axis lists the feature names.\n",
    "     - The x-axis represents the importance score of each feature.\n",
    "   - Features with higher importance scores appear at the top, making it easy to identify the most influential features.\n",
    "\n",
    "3. **Customization**:\n",
    "   - The `plot_importance` function provides several optional parameters (e.g., `title`, `xlabel`, `ylabel`, `max_num_features`) to customize the plot. However, in this case, the default settings are used.\n",
    "\n",
    "4. **Purpose**:\n",
    "   - This visualization is particularly useful for understanding the model's behavior and for feature selection. For example, less important features might be removed to simplify the model or improve computational efficiency.\n",
    "\n",
    "The `# type: ignore` comment suppresses type-checking warnings, likely because the `plot_importance` function may not have explicit type annotations compatible with the user's type-checking tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1675033855592,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "lLtgBOFOS2JB",
    "nbgrader": {
     "checksum": "37d4bbbe63ad4695e6e10a595a2d4d6a",
     "grade": false,
     "grade_id": "cell-bab595f4798d54dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "6e73c0b2-fa71-463e-eecc-32fe0ab90475"
   },
   "outputs": [],
   "source": [
    "xgboost.plot_importance(clf) # type: ignore"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

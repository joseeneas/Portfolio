{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code contains commented-out commands intended for use in Google Colab, a cloud-based Jupyter notebook environment. These commands are used to manage the installation and upgrading of necessary Python libraries. Here's a detailed explanation of each command:\n",
    "\n",
    "1. `#%pip uninstall -y numpy pandas scipy scikit-learn imbalanced-learn`: This command uninstalls the specified libraries (`numpy`, `pandas`, `scipy`, `scikit-learn`, and `imbalanced-learn`) without prompting for confirmation (`-y` flag). This might be useful if you need to remove existing versions before installing specific versions.\n",
    "\n",
    "2. `#%pip install numpy pandas scipy scikit-learn imbalanced-learn`: This command installs the specified libraries. These libraries are essential for data manipulation (`numpy`, `pandas`), scientific computing (`scipy`), machine learning (`scikit-learn`), and handling imbalanced datasets (`imbalanced-learn`).\n",
    "\n",
    "3. `#%pip install seaborn`: This command installs the `seaborn` library, which is used for statistical data visualization.\n",
    "\n",
    "4. `#%pip install skimpy`: This command installs the `skimpy` library, which is used for quick and easy data exploration.\n",
    "\n",
    "5. `#%pip install --upgrade numpy scikit-learn`: This command upgrades the `numpy` and `scikit-learn` libraries to their latest versions.\n",
    "\n",
    "6. `#from google.colab import drive`: This command imports the `drive` module from `google.colab`, which is used to interact with Google Drive.\n",
    "\n",
    "7. `#drive.mount('/content/drive')`: This command mounts Google Drive to the Colab environment, allowing you to access files stored in your Google Drive account.\n",
    "\n",
    "These commands are commented out, indicating that they are not currently being executed. They are useful for setting up the environment when running the notebook in Google Colab, ensuring that all required libraries are installed and up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34385,
     "status": "ok",
     "timestamp": 1741203215010,
     "user": {
      "displayName": "Jose Eneas Da Silva Maria",
      "userId": "05457363209835800857"
     },
     "user_tz": 300
    },
    "id": "rOPsaJbaSOv2",
    "outputId": "226e4846-4ebd-40ea-e48c-422da8f7f1a3"
   },
   "outputs": [],
   "source": [
    "#%pip uninstall -y numpy pandas scipy scikit-learn imbalanced-learn\n",
    "#%pip install numpy pandas scipy scikit-learn imbalanced-learn\n",
    "#%pip install seaborn\n",
    "#%pip install skimpy\n",
    "#%pip install --upgrade numpy scikit-learn\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports various libraries and modules that are essential for performing data analysis, preprocessing, and logistic regression modeling in Python. Here's a breakdown of each import and its purpose:\n",
    "\n",
    "1. `warnings`: This module is used to manage warnings in Python. It allows you to control whether warnings are ignored, displayed, or turned into errors.\n",
    "\n",
    "2. `pandas as pd`: Pandas is a powerful data manipulation and analysis library. It provides data structures like DataFrames, which are essential for handling and analyzing structured data.\n",
    "\n",
    "3. `matplotlib.pyplot as plt`: Matplotlib is a plotting library, and `pyplot` is a module within it that provides a MATLAB-like interface for creating static, interactive, and animated visualizations in Python.\n",
    "\n",
    "4. `seaborn as sns`: Seaborn is a statistical data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "5. `skimpy import skim`: Skimpy is a library used for quick and easy data exploration. The `skim` function provides a summary of a DataFrame, similar to the `skimr` package in R.\n",
    "\n",
    "6. `numpy as np`: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and many mathematical functions.\n",
    "\n",
    "7. `from sklearn.preprocessing import StandardScaler`: This module from scikit-learn is used for feature scaling. `StandardScaler` standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "8. `from imblearn.over_sampling import SMOTE`: SMOTE (Synthetic Minority Over-sampling Technique) is used to handle imbalanced datasets by generating synthetic samples for the minority class.\n",
    "\n",
    "9. `import statsmodels.api as sm`: Statsmodels is a library for estimating and testing statistical models. It provides classes and functions for many statistical models, including linear regression, logistic regression, and time series analysis.\n",
    "\n",
    "10. `from sklearn.model_selection import train_test_split`: This function from scikit-learn is used to split a dataset into training and testing sets.\n",
    "\n",
    "11. `from statsmodels.tools.sm_exceptions import ConvergenceWarning`: This import is used to handle convergence warnings that may arise during the fitting of statistical models in statsmodels.\n",
    "\n",
    "12. `from sklearn.linear_model import LogisticRegression`: This module from scikit-learn provides the `LogisticRegression` class, which is used to perform logistic regression.\n",
    "\n",
    "13. `from sklearn.metrics import accuracy_score, confusion_matrix`: These functions from scikit-learn are used to evaluate the performance of a classification model. `accuracy_score` calculates the accuracy of the model, and `confusion_matrix` provides a summary of prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install skimpy\n",
    "%pip install imblearn\n",
    "import warnings  \n",
    "import pandas                          as pd # type: ignore\n",
    "import matplotlib.pyplot               as plt # type: ignore\n",
    "import seaborn                         as sns # type: ignore\n",
    "from   skimpy                          import skim # type: ignore\n",
    "import numpy                           as     np # type: ignore\n",
    "from   imblearn.over_sampling          import SMOTE # type: ignore\n",
    "from   sklearn.model_selection         import train_test_split # type: ignore\n",
    "from   sklearn.linear_model            import LogisticRegression # type: ignore\n",
    "from   sklearn.metrics                 import accuracy_score, confusion_matrix # type: ignore\n",
    "from   sklearn.preprocessing           import StandardScaler # type: ignore\n",
    "import statsmodels.api                 as     sm # type: ignore\n",
    "from   statsmodels.tools.sm_exceptions import ConvergenceWarning # type: ignore\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "np.seterr(over='ignore', invalid='ignore')\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs several key tasks related to data loading and initial exploration using the Pandas library in Python.\n",
    "\n",
    "First, the code reads data from an Excel file named 'Logit-Data.xlsx' into a DataFrame `df` using the `pd.read_excel` function. This function is highly versatile and can handle various parameters to customize the data reading process, but in this case, it is used with its default settings.\n",
    "\n",
    "Next, the code adjusts several display options to enhance the readability of the DataFrame when printed. The `pd.set_option` function is used to:\n",
    "- Display all columns of the DataFrame by setting `display.max_columns` to `None`.\n",
    "- Set the display width to 1000 characters for better readability by adjusting `display.width`.\n",
    "- Limit the maximum column width to 50 characters using `display.max_colwidth`, ensuring that long text entries do not overwhelm the display.\n",
    "- Adjust the maximum number of rows to show to 20 using `display.max_rows`, which helps in managing the output size when printing large DataFrames.\n",
    "\n",
    "After setting these display options, the code verifies the data by printing the first and last few rows of the DataFrame and generating summary statistics:\n",
    "- `print(df.head(5))` prints the first 5 rows of the DataFrame, providing a quick look at the beginning of the dataset.\n",
    "- `print(df.tail(5))` prints the last 5 rows of the DataFrame, offering a glimpse of the end of the dataset.\n",
    "- `print(df.describe())` generates and prints summary statistics for the numerical columns in the DataFrame, including count, mean, standard deviation, minimum, maximum, and specified percentiles (25th, 50th, and 75th).\n",
    "\n",
    "These steps are crucial for initial data exploration, allowing you to understand the structure, content, and basic statistics of the dataset before proceeding with further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 4318,
     "status": "error",
     "timestamp": 1741203669863,
     "user": {
      "displayName": "Jose Eneas Da Silva Maria",
      "userId": "05457363209835800857"
     },
     "user_tz": 300
    },
    "id": "MyTU2KPzqMPs",
    "outputId": "a481cb37-8b78-460d-ed0d-e2a9bd3c0d7f"
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "df = pd.read_excel('files/Logit-Data.xlsx')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)       \n",
    "pd.set_option('display.max_colwidth', 50)   \n",
    "pd.set_option('display.max_rows', 20)      \n",
    "\n",
    "print(df.head(5))   \n",
    "print(df.tail(5))\n",
    "print(df.describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `skim(df)` function call is used to generate a comprehensive visual summary of the DataFrame `df`. This function is part of the `skimpy` library, which provides an alternative to the traditional `pandas.DataFrame.describe()` method by offering a more detailed and visually appealing overview of the data.\n",
    "\n",
    "The `skim` function takes a DataFrame as input and produces a summary table that is displayed in the console. This summary includes various statistics and information about each column in the DataFrame, tailored to the specific data types of the columns. For example, it might show the distribution of values, the number of missing values, and other relevant metrics.\n",
    "\n",
    "The function is designed to handle both Pandas and Polars DataFrames, making it versatile for different data manipulation libraries. However, it does not support DataFrames with multi-column indexes, and it is recommended to ensure that the data types of the columns are correctly set before running the function to get the best results.\n",
    "\n",
    "By using `skim(df)`, you can quickly gain insights into the structure and content of your dataset, which is particularly useful during the initial stages of data exploration and analysis. This helps in identifying potential issues, such as missing values or incorrect data types, and provides a solid foundation for further data processing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1739336697462,
     "user": {
      "displayName": "Hatice Kuey",
      "userId": "00862202582568672734"
     },
     "user_tz": 360
    },
    "id": "Ej8oDS5gpWC6",
    "outputId": "72eaedbb-84c6-4e01-e196-edc71b49673b"
   },
   "outputs": [],
   "source": [
    "skim(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the `skim` function from the `skimpy` library to generate visual summary statistics for two subsets of the DataFrame `df`. The subsets are created based on the values in the 'default' column, which likely indicates whether a certain condition (such as loan default) is met.\n",
    "\n",
    "1. `skim(df[df['default'] == 0])`: This line filters the DataFrame `df` to include only the rows where the 'default' column has a value of 0. This subset represents the population that did not default. The `skim` function is then called on this subset to provide a detailed summary of its structure and content. This summary includes various statistics and information about each column, helping to understand the characteristics of the non-defaulting population.\n",
    "\n",
    "2. `skim(df[df['default'] == 1])`: Similarly, this line filters the DataFrame `df` to include only the rows where the 'default' column has a value of 1. This subset represents the population that did default. The `skim` function is called on this subset to generate a visual summary, offering insights into the characteristics of the defaulting population.\n",
    "\n",
    "By skimming these two subsets separately, you can compare the summary statistics and distributions of the features for the defaulting and non-defaulting populations. This comparison can reveal important differences and patterns that may be useful for further analysis, such as identifying risk factors associated with default. The `skim` function provides a quick and comprehensive overview, making it easier to spot trends and anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1739336700009,
     "user": {
      "displayName": "Hatice Kuey",
      "userId": "00862202582568672734"
     },
     "user_tz": 360
    },
    "id": "f5XSwUWnqlF-",
    "outputId": "4ff65ece-95e6-4ab7-cf44-1a47e7ea3657"
   },
   "outputs": [],
   "source": [
    "# Skim the population with default = 0\n",
    "skim(df[df['default'] == 0])\n",
    "# Skim the population with default = 1\n",
    "skim(df[df['default'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code `print(df['default'].value_counts())` is used to display the frequency of unique values in the 'default' column of the DataFrame `df`. This operation is particularly useful for understanding the distribution of categorical data within the column. Here’s a detailed explanation of each component:\n",
    "\n",
    "1. **`df['default']`**: This part of the code accesses the 'default' column in the DataFrame `df`. The 'default' column likely contains categorical data, such as binary values (e.g., 0 and 1) indicating whether a certain condition, like loan default, is met.\n",
    "\n",
    "2. **`value_counts()`**: This method is called on the 'default' column. The `value_counts()` method counts the occurrences of each unique value in the column. By default, it returns a Series sorted by the counts in descending order. This method is useful for quickly summarizing the distribution of categorical data.\n",
    "\n",
    "3. **`print()`**: The `print()` function outputs the result of the `value_counts()` method to the console. The `print()` function takes the Series returned by `value_counts()` and displays it in a readable format.\n",
    "\n",
    "The output of this code will show the number of times each unique value appears in the 'default' column. For example, if the 'default' column contains binary values (0 and 1), the output might look something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['default'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs two main tasks: displaying the first few rows of the DataFrame `df` and visualizing the distribution of specific features using histograms.\n",
    "\n",
    "1. **Displaying the First Few Rows**:\n",
    "   - `print(df.head())`: This line prints the first five rows of the DataFrame `df`. The `head()` method is used to quickly inspect the initial entries of the DataFrame, providing a snapshot of the data structure and content. This is useful for verifying that the data has been loaded correctly and for getting an initial sense of the dataset.\n",
    "\n",
    "2. **Visualizing Feature Distributions**:\n",
    "   - The variable `features` is defined as a list containing the names of four columns: `'fico_score'`, `'log_income'`, `'installment'`, and `'rev_balance'`. These columns represent specific features in the dataset that you want to analyze.\n",
    "   - A `for` loop iterates over each feature in the `features` list. For each feature, the following steps are performed:\n",
    "     - `sns.histplot(df[feature], kde=True)`: This line creates a histogram of the current feature using the Seaborn library. The `kde=True` parameter adds a Kernel Density Estimate (KDE) curve to the histogram, which provides a smoothed estimate of the feature's distribution. This combination of histogram and KDE curve helps in understanding the distribution shape and density of the feature values.\n",
    "     - `plt.title(f'Distribution of {feature}')`: This line sets the title of the plot to indicate which feature's distribution is being visualized. The `f-string` syntax (`f'Distribution of {feature}'`) dynamically inserts the feature name into the title.\n",
    "     - `plt.show()`: This line displays the plot. It ensures that each histogram is rendered and shown before the next iteration of the loop begins.\n",
    "\n",
    "By executing this code, you can visually inspect the distribution of the specified features in the dataset. This is an important step in exploratory data analysis (EDA), as it helps identify patterns, outliers, and potential issues such as skewness or multimodality in the data. Understanding the distribution of features is crucial for making informed decisions about data preprocessing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6264,
     "status": "ok",
     "timestamp": 1740083233587,
     "user": {
      "displayName": "Pablo Aguirre Solana",
      "userId": "09295983483986446252"
     },
     "user_tz": 360
    },
    "id": "FKrc32OSvVRR",
    "outputId": "30e11507-3bf4-45f5-f0ca-bd503cddaf8a"
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "\n",
    "features = ['fico_score', 'log_income', 'installment', 'rev_balance']\n",
    "for feature in features:\n",
    "    sns.histplot(df[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs several important tasks related to data preprocessing and handling class imbalance in a dataset using the Synthetic Minority Over-sampling Technique (SMOTE). Here's a detailed explanation of each step:\n",
    "\n",
    "1. **Displaying the First Few Rows**:\n",
    "   - This line prints the first five rows of the DataFrame `df`. The generic.py ) method is used to quickly inspect the initial entries of the DataFrame, providing a snapshot of the data structure and content. This is useful for verifying that the data has been loaded correctly and for getting an initial sense of the dataset.\n",
    "\n",
    "2. **Separating Features and Target Variable**:\n",
    "   - `X = df.drop('default', axis=1)`: This line creates a new DataFrame[`X` by dropping the 'default' column from the original DataFrame `df`. The 'default' column is assumed to be the target variable, and the remaining columns are the features.\n",
    "   - `y = df['default']`: This line extracts the 'default' column from the DataFrame `df` and assigns it to the variable `y`. This variable represents the target variable for the classification task.\n",
    "\n",
    "3. **Applying SMOTE for Class Imbalance**:\n",
    "   - `smote = SMOTE()`: This line initializes an instance of the SMOTE class from the `imblearn` library. SMOTE is a technique used to address class imbalance by generating synthetic samples for the minority class.\n",
    "   - `X_res, y_res = smote.fit_resample(X, y)`: This line applies the SMOTE algorithm to the feature matrix `X` and target variable `y`. The `fit_resample()` method returns the resampled feature matrix `X_res` and target variable `y_res`, where the class distribution is balanced.\n",
    "\n",
    "4. **Displaying the Resampled Class Distribution**:\n",
    "   - This line prints the count of unique values in the resampled target variable `y_res`. The `value_counts` method counts the occurrences of each unique value in the Series and returns a Series sorted by the counts in descending order. This output helps verify that the class imbalance has been addressed by showing the distribution of the target variable after applying SMOTE.\n",
    "\n",
    "By executing this code, you can ensure that the dataset is balanced, which is crucial for training machine learning models, especially in classification tasks where class imbalance can lead to biased models. SMOTE helps in generating a more balanced dataset by creating synthetic samples for the minority class, improving the model's ability to learn from both classes effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1852,
     "status": "ok",
     "timestamp": 1740083282254,
     "user": {
      "displayName": "Pablo Aguirre Solana",
      "userId": "09295983483986446252"
     },
     "user_tz": 360
    },
    "id": "dSk2yEk-wVsx",
    "outputId": "18fc84d0-6e21-44d7-90bc-a735567494fe"
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "X = df.drop('default', axis=1)\n",
    "y = df['default']\n",
    "smote = SMOTE()\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "print(y_res.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs several key steps in preparing and fitting a logistic regression model using the `statsmodels` library in Python. Here's a detailed explanation of each step:\n",
    "\n",
    "1. **Splitting the Data**:\n",
    "   - `X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2, _split`: This line splits the resampled feature matrix (`X_res`) and target variable (`y_res`) into training and validation sets. The `train_test_split` function from `sklearn.model_selection` is used to randomly split the data. The `test_size=0.2` parameter specifies that 20% of the data should be used for validation, while the remaining 80% is used for training. The `random_state=42` parameter ensures that the split is reproducible.\n",
    "\n",
    "2. **Adding a Constant Term**:\n",
    "   - `X_train_sm =`: This line adds a constant term (intercept) to the training feature matrix (`X_train`). The `add_constant` function from `statsmodels` is used to add a column of ones to the feature matrix, which is necessary for fitting an intercept in the logistic regression model.\n",
    "\n",
    "3. **Defining the Logistic Regression Model**:\n",
    "   - `logit_model = sm.Logit(y_train, X_train_sm, method='bfgs'`: This line defines a logistic regression model using the `Logit` class from `statsmodels`. The model is specified with the training target variable (`y_train`) and the training feature matrix with the added constant (`X_train_sm`). The `method='bfgs'` parameter specifies that the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm should be used for optimization. The `maxiter=2000` parameter sets the maximum number of iterations for the optimization algorithm to 2000.\n",
    "\n",
    "4. **Fitting the Model**:\n",
    "   - `result = logit_model.fit(max_iter=2000)`: This line fits the logistic regression model to the training data. The `fit` method is called on the `logit_model` object, and the `max_iter=2000` parameter ensures that the optimization algorithm has sufficient iterations to converge. The result of the fitting process is stored in the `result` object, which contains various details about the fitted model.\n",
    "\n",
    "5. **Displaying the Model Summary**:\n",
    "   - This line prints a summary of the fitted logistic regression model. The `summary` method of the `result` object generates a detailed report that includes information such as the coefficients of the model, standard errors, z-values, p-values, and various goodness-of-fit statistics. This summary is useful for interpreting the results of the logistic regression analysis and understanding the significance of the predictors.\n",
    "\n",
    "By executing this code, you can train a logistic regression model on the resampled dataset, ensuring that the model accounts for class imbalance. The model summary provides valuable insights into the relationships between the predictors and the target variable, helping you to evaluate the model's performance and interpret its coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3002,
     "status": "ok",
     "timestamp": 1740083444004,
     "user": {
      "displayName": "Pablo Aguirre Solana",
      "userId": "09295983483986446252"
     },
     "user_tz": 360
    },
    "id": "8eLsU8n_Dh-i",
    "outputId": "a2d1dbec-f5f4-4340-e1c3-910ab4e84e2b"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "logit_model = sm.Logit(y_train, X_train_sm, method='bfgs', maxiter=2000)\n",
    "result = logit_model.fit(max_iter=2000)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs several key steps in training and evaluating a logistic regression model using the `scikit-learn` library in Python. Here's a detailed explanation of each step:\n",
    "\n",
    "1. **Splitting the Data**:\n",
    "   - `X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2`: This line splits the resampled feature matrix (`X_res`) and target variable (`y_res`) into training and validation sets. The `train_test_split` function from `sklearn.model_selection` is used to randomly split the data. The `test_size=0.2` parameter specifies that 20% of the data should be used for validation, while the remaining 80% is used for training. The `random_state=42` parameter ensures that the split is reproducible.\n",
    "\n",
    "2. **Defining and Training the Logistic Regression Model**:\n",
    "   - `model = `: This line initializes a logistic regression model using the `LogisticRegression` class from `scikit-learn`. The `max_iter=1000` parameter sets the maximum number of iterations for the optimization algorithm to 1000, ensuring that the model has sufficient iterations to converge.\n",
    "   - `model.fit(X_train,y_train`: This line fits the logistic regression model to the training data. The `fit` method is called on the `model` object, using the training feature matrix (`X_train`) and the training target variable (`y_train`). This step trains the model by finding the optimal coefficients that minimize the logistic loss function.\n",
    "\n",
    "3. **Making Predictions**:\n",
    "   - `y_pred =`: This line uses the trained logistic regression model to make predictions on the validation set. The `predict` method is called on the `model` object, using the validation feature matrix (`X_val`). The predicted labels are stored in the `y_pred` variable.\n",
    "\n",
    "4. **Evaluating the Model**:\n",
    "   - `print(\"Accuracy:\", accuracy_score(y_val,y_pred)`: This line calculates and prints the accuracy of the model on the validation set. The `accuracy_score` function from `sklearn.metrics` is used to compute the accuracy, which is the fraction of correctly classified samples. The true labels (`y_val`) and the predicted labels (`y_pred`) are passed as arguments to the function.\n",
    "   - `print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred)`: This line calculates and prints the confusion matrix for the validation set. The `confusion_matrix` function from `sklearn.metrics` is used to compute the confusion matrix, which provides a summary of the prediction results by showing the counts of true positives, true negatives, false positives, and false negatives. The true labels (`y_val`) and the predicted labels (`y_pred`) are passed as arguments to the function.\n",
    "\n",
    "By executing this code, you can train a logistic regression model on the resampled dataset, make predictions on the validation set, and evaluate the model's performance using accuracy and the confusion matrix. These evaluation metrics provide insights into how well the model is performing and help identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1740083301101,
     "user": {
      "displayName": "Pablo Aguirre Solana",
      "userId": "09295983483986446252"
     },
     "user_tz": 360
    },
    "id": "mraEV2-pwhYw",
    "outputId": "6ac81fdd-268e-42e8-f42a-b4a2905b8e2b"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "# Print the accuracy. what percentage of the validation set was correctly classified?. \n",
    "# The accuracy is the ratio of the number of correct predictions to the total number of predictions.\n",
    "# 59% of the validation set was correctly classified. Which is not bad considering the imbalance in the dataset.\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs several key steps in preprocessing the data, training a logistic regression model, and evaluating its performance using the `scikit-learn` library in Python. Here's a detailed explanation of each step:\n",
    "\n",
    "1. **Standardizing the Data**:\n",
    "   - `scaler =`: This line initializes an instance of the `StandardScaler` class from `scikit-learn`. The `StandardScaler` standardizes features by removing the mean and scaling to unit variance, which is a common preprocessing step for many machine learning algorithms.\n",
    "   - `X_train_scaled =`: This line fits the `StandardScaler` to the training data (`X_train`) and then transforms it. The `fit_transform` method computes the mean and standard deviation of the training data and scales it accordingly.\n",
    "   - `X_val_scaled =`: This line transforms the validation data (`X_val`) using the same scaling parameters computed from the training data. The `transform` method scales the validation data based on the mean and standard deviation of the training data, ensuring that both datasets are standardized in the same way.\n",
    "\n",
    "2. **Training the Logistic Regression Model**:\n",
    "   - `model =`: This line initializes a logistic regression model using the `LogisticRegression` class from `scikit-learn`. The `max_iter=1000` parameter sets the maximum number of iterations for the optimization algorithm to 1000, ensuring that the model has sufficient iterations to converge.\n",
    "   - `model.fit(X_train_scaled,`: This line fits the logistic regression model to the standardized training data (`X_train_scaled`) and the training target variable (`y_train`). The `fit` method trains the model by finding the optimal coefficients that minimize the logistic loss function.\n",
    "\n",
    "3. **Making Predictions**:\n",
    "   - `y_pred =`: This line uses the trained logistic regression model to make predictions on the standardized validation data (`X_val_scaled`). The `predict` method is called on the `model` object, and the predicted labels are stored in the `y_pred` variable.\n",
    "\n",
    "4. **Evaluating the Model**:\n",
    "   - `print(\"Accuracy:\", accuracy_score(y_val,`: This line calculates and prints the accuracy of the model on the validation set. The `accuracy_score` function from `sklearn.metrics` is used to compute the accuracy, which is the fraction of correctly classified samples. The true labels (`y_val`) and the predicted labels (`y_pred`) are passed as arguments to the function.\n",
    "   - `print(\"Confusion Matrix:\\n\", confusion_matrix(y_val,`: This line calculates and prints the confusion matrix for the validation set. The `confusion_matrix` function from `sklearn.metrics` is used to compute the confusion matrix, which provides a summary of the prediction results by showing the counts of true positives, true negatives, false positives, and false negatives. The true labels (`y_val`) and the predicted labels (`y_pred`) are passed as arguments to the function.\n",
    "\n",
    "5. **Displaying the First Few Rows of the DataFrame**:\n",
    "   - This line prints the first six rows of the DataFrame (`df`). The `head` method is used to quickly inspect the initial entries of the DataFrame, providing a snapshot of the data structure and content.\n",
    "\n",
    "By executing this code, you can preprocess the data by standardizing it, train a logistic regression model on the standardized training data, make predictions on the standardized validation data, and evaluate the model's performance using accuracy and the confusion matrix. These evaluation metrics provide insights into how well the model is performing and help identify areas for improvement. Additionally, displaying the first few rows of the DataFrame helps verify that the data has been loaded correctly and provides an initial sense of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I9jm7KmwsEs"
   },
   "outputs": [],
   "source": [
    "scaler         = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "model          = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred         = model.predict(X_val_scaled)\n",
    "# Print the accuracy. what percentage of the validation set was correctly classified?. \n",
    "# The accuracy is the ratio of the number of correct predictions to the total number of predictions.\n",
    "# 59% of the validation set was correctly classified. Which is not bad considering the imbalance in the dataset.\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "print(df.head(6))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1GP3mLxRc0u3UuekWvG7_72ktmsZpKTfs",
     "timestamp": 1741205606280
    },
    {
     "file_id": "1aYcNxa5tcJ6x19l3pYrRMz0ZUh2Qo2SB",
     "timestamp": 1651767952568
    },
    {
     "file_id": "1Pcz0iI2GAHs1lnZ-iD34ScEo-GokSHdq",
     "timestamp": 1651761890037
    }
   ]
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pLgwRLZiMVJv",
    "nbgrader": {
     "checksum": "4b220c408d9ad83ded4ce03d707e956f",
     "grade": false,
     "grade_id": "cell-6ff916a5697a4710",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Deep Learning for Text - Encoder Models\n",
    "## Data Preprocessing\n",
    "\n",
    "This code below demonstrates how to suppress various types of warning messages in Python using the built-in `warnings` module. This is a common practice in data science and machine learning projects where libraries often generate numerous warnings that can clutter the output without indicating actual problems.\n",
    "\n",
    "## Warning Suppression Strategy\n",
    "\n",
    "The code uses `warnings.simplefilter()` with the `action=\"ignore\"` parameter to completely suppress different categories of warnings. Each call targets a specific warning type: `FutureWarning` (alerts about upcoming changes in library behavior), `UserWarning` (general warnings for users), `RuntimeWarning` (warnings about dubious runtime behavior), and `DeprecationWarning` (notifications about deprecated features). By ignoring these warnings, the notebook output becomes cleaner and focuses on the actual results rather than being overwhelmed by non-critical messages.\n",
    "\n",
    "## When and Why to Use Warning Suppression\n",
    "\n",
    "This approach is particularly valuable in educational or demonstration contexts, like this deep learning assignment, where the focus should be on understanding the core concepts rather than dealing with library maintenance issues. Many machine learning libraries like TensorFlow, scikit-learn, and pandas frequently emit warnings about future API changes or deprecated functions, which can be distracting when learning. However, it's important to note that suppressing warnings should be done judiciously - in production code, you'd typically want to address the underlying causes of warnings rather than simply hiding them.\n",
    "\n",
    "## Best Practices and Considerations\n",
    "\n",
    "While this blanket suppression is convenient for notebooks and learning environments, **be cautious about using this in production code**. Warnings often provide valuable information about potential issues or upcoming breaking changes that could affect your application. A better practice in production would be to address specific warnings individually or use more targeted filtering. Additionally, consider that suppressing `DeprecationWarning` might mask important information about functions that will be removed in future library versions, potentially leading to code that breaks when dependencies are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SyntaxWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=ImportWarning)  \n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xgv9-wh2i-eg",
    "nbgrader": {
     "checksum": "e841f6255d99649ca03496667ea4fe2f",
     "grade": false,
     "grade_id": "cell-4b987f312bc09df1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The code below demonstrates hardware detection and Metal GPU configuration for TensorFlow on Apple Silicon (M1/M2) machines. It's a system diagnostic that checks available compute devices and attempts to configure GPU memory settings.\n",
    "\n",
    "## Device Discovery and Backend Information\n",
    "\n",
    "The code begins by importing TensorFlow and the Keras backend module, then uses `tf.config.list_physical_devices()` to discover all available hardware devices (CPUs, GPUs, etc.) on the system. The `GPUs = tf.config.list_physical_devices()` line captures all physical devices for later use, while `backend.backend()` confirms which backend Keras is using (typically \"tensorflow\"). This information is crucial for understanding what computational resources are available and ensuring the correct backend is loaded.\n",
    "\n",
    "## Metal GPU Configuration Attempt\n",
    "\n",
    "The try-except block attempts to configure Apple's Metal Performance Shaders (MPS) backend, which allows TensorFlow to leverage the GPU on Apple Silicon chips. The `tf.device('/metal:GPU')` context manager tries to target the Metal GPU specifically. Inside this context, the code checks if memory growth is enabled using `tf.config.experimental.get_memory_growth(GPUs[0])`. Memory growth is a setting that allows TensorFlow to allocate GPU memory incrementally rather than claiming all available memory at startup, which is especially important on unified memory architectures like Apple Silicon.\n",
    "\n",
    "## Error Handling and Diagnostics\n",
    "\n",
    "The exception handling is particularly important here because Metal GPU support isn't available on all systems and configurations. If the Metal GPU isn't accessible (perhaps because it's not an Apple Silicon machine, Metal support isn't properly installed, or the device is already in use), the exception will be caught and a diagnostic message printed. This graceful degradation ensures the notebook can continue running even if the optimal GPU acceleration isn't available.\n",
    "\n",
    "**Key gotcha**: The code assumes at least one physical device exists when accessing `GPUs[0]`. On systems with no GPUs, this could raise an IndexError. Also, memory growth settings typically need to be configured before any TensorFlow operations are performed, so this diagnostic code should run early in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet --upgrade pip\n",
    "#%pip install --quiet --upgrade keras\n",
    "#%pip install --quiet --upgrade tensorflow\n",
    "#%pip install --quiet --upgrade tensorflow-macos\n",
    "#%pip install --quiet --upgrade tensorflow-metal\n",
    "#%pip install --quiet --upgrade scikit-learn\n",
    "#%pip install --quiet --upgrade pydot\n",
    "#%pip install --quiet --upgrade graphviz\n",
    "import tensorflow as     tf  \n",
    "from   keras      import backend   \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print('=== TensorFlow Environment Summary ===')\n",
    "print('Operating System          :', os.name )\n",
    "print('Platform Version          :', os.sys.platform )\n",
    "print('Operating System Version  :', os.uname().version if hasattr(os, 'uname') else 'N/A')\n",
    "print('TensorFlow version        :', tf.__version__)\n",
    "print('Keras version             :', tf.keras.__version__)\n",
    "print('Python version            :', os.sys.version)\n",
    "print('Python executable         :', os.sys.executable)\n",
    "print('Python path               :', os.sys.path)\n",
    "print('TensorFlow backend        :', backend.backend())\n",
    "print('TensorFlow device list    :', tf.config.list_physical_devices())\n",
    "print('TensorFlow eager execution:', tf.executing_eagerly())\n",
    "print('TensorFlow GPU available  :', tf.config.list_physical_devices('GPU'))\n",
    "print('TensorFlow TPU available  :', tf.config.list_physical_devices('TPU'))\n",
    "print('VS Code Version           :', os.environ.get('TERM_PROGRAM_VERSION', 'N/A'))\n",
    "all_devices = tf.config.list_physical_devices()\n",
    "print('=== Device Discovery Summary ===')\n",
    "print(f'Total devices found: {len(all_devices)}')\n",
    "print()\n",
    "\n",
    "# Group devices by type\n",
    "device_types = {}\n",
    "for device in all_devices:\n",
    "    device_type = device.device_type\n",
    "    if device_type not in device_types:\n",
    "        device_types[device_type] = []\n",
    "    device_types[device_type].append(device)\n",
    "\n",
    "# Display each device type separately\n",
    "for device_type, devices in device_types.items():\n",
    "    print(f'{device_type} Devices ({len(devices)} found):')\n",
    "    for i, device in enumerate(devices):\n",
    "        print(f'  [{i}] {device.name}')\n",
    "    print()\n",
    "\n",
    "# Alternative: Query specific device types\n",
    "print('=== Detailed Device Breakdown ===')\n",
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(f'CPU devices: {len(cpu_devices)}')\n",
    "for i, cpu in enumerate(cpu_devices):\n",
    "    print(f'  CPU[{i}]: {cpu.name}')\n",
    "\n",
    "print(f'GPU devices: {len(gpu_devices)}')\n",
    "for i, gpu in enumerate(gpu_devices):\n",
    "    print(f'  GPU[{i}]: {gpu.name}')\n",
    "    \n",
    "# Check for other common device types\n",
    "try:\n",
    "    tpu_devices = tf.config.list_physical_devices('TPU')\n",
    "    if tpu_devices:\n",
    "        print(f'TPU devices: {len(tpu_devices)}')\n",
    "        for i, tpu in enumerate(tpu_devices):\n",
    "            print(f'  TPU[{i}]: {tpu.name}')\n",
    "except Exception:\n",
    "    print('TPU devices: Not available')\n",
    "\n",
    "# Get GPU devices specifically\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print('GPU devices found:', len(gpu_devices))\n",
    "if gpu_devices:\n",
    "    try:\n",
    "        # Check memory growth for first GPU\n",
    "        memory_growth = tf.config.experimental.get_memory_growth(gpu_devices[0])\n",
    "        print(f\"Memory growth enabled: {memory_growth}\")\n",
    "        \n",
    "        # Try Metal GPU access\n",
    "        with tf.device(':GPU:0'):\n",
    "            print(\"Successfully accessed GPU device\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"GPU access error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates the essential imports and reproducibility setup for a machine learning project using TensorFlow and Keras. It establishes the foundation for data manipulation, deep learning model development, and ensures consistent results across multiple runs.\n",
    "\n",
    "## Core Library Imports\n",
    "\n",
    "The code imports the fundamental libraries needed for machine learning workflows. `numpy` provides the foundation for numerical computing with efficient array operations, while `pandas` offers powerful data manipulation and analysis capabilities through DataFrames and Series. The `tensorflow` import brings in the main deep learning framework, and `from tensorflow import keras` specifically imports Keras, which is TensorFlow's high-level API for building and training neural networks. This import structure is standard practice in modern TensorFlow projects where Keras serves as the primary interface for model development.\n",
    "\n",
    "## Reproducibility Through Seed Setting\n",
    "\n",
    "The `tf.random.set_seed(42)` call is crucial for ensuring reproducible results in machine learning experiments. This sets the global random seed for TensorFlow operations, which means that any random processes in your model (like weight initialization, dropout, or data shuffling) will produce the same sequence of \"random\" numbers each time you run the code. The choice of 42 is arbitrary but commonly used in programming examples. This reproducibility is essential for debugging, comparing model performance, and ensuring that research results can be replicated.\n",
    "\n",
    "## Importance in Machine Learning Workflows\n",
    "\n",
    "Setting a random seed is particularly important in deep learning because neural networks rely heavily on randomness - from initial weight values to training batch order. Without a fixed seed, you might get different results each time you train the same model, making it difficult to determine whether performance improvements come from actual model changes or just lucky random initialization. This is especially critical in educational contexts like this assignment, where consistent results help students understand the impact of different techniques and hyperparameters.\n",
    "\n",
    "**Key consideration**: While setting seeds ensures reproducibility, remember that this only works within the same hardware and software environment. Results may still vary across different versions of TensorFlow or when switching between CPU and GPU execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 3006,
     "status": "ok",
     "timestamp": 1677214904290,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "7rJh-xXx4jgD",
    "nbgrader": {
     "checksum": "ec7d490bb52dc1759e267746f769ef91",
     "grade": false,
     "grade_id": "cell-ca423d34261a7199",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy          as np\n",
    "import pandas         as pd\n",
    "import tensorflow     as tf\n",
    "from   tensorflow import keras\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin extracting the data from the ATIS dataset and turning into a form that we can use in our Deep Learning models.\n",
    "\n",
    "The ATIS dataset is standard benchmark dataset widely used to build models for intent classification and slot filling tasks (we will explain all this shortly). You can find a very detailed explanation [here](https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html).\n",
    "\n",
    "We will begin by loading the file and then partitioning into a test and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "782328bb16467f10a024c47e7cb196cc",
     "grade": false,
     "grade_id": "cell-1f82a84e23534653",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('files/atis_train_data.csv') # type: ignore\n",
    "df_test = pd.read_csv('files/atis_test_data.csv') # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WBciCu0YP_Sg",
    "nbgrader": {
     "checksum": "1ed359018f4424875a6d269949e54a69",
     "grade": false,
     "grade_id": "cell-5d04969236981a71",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's visualize all of this on a dataframe. Below we display an example query for each intent class in a nice layout.\n",
    "\n",
    "The first column of the Dataframe below contains the actual query that was asked. The second column indicates the intent (flight, flight time, etc), whereas the last column contains the slot filling structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "00214ef688603776717a26fb40b10e0b",
     "grade": false,
     "grade_id": "cell-ab7aa1b0a9b6bcfe",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # type: ignore\n",
    "df_small = pd.DataFrame(columns=['query','intent','slot filling']) # type: ignore\n",
    "j        = 0\n",
    "for i in df_train.intent.unique(): # type: ignore\n",
    "  df_small.loc[j] = df_train[df_train.intent==i].iloc[0] # type: ignore\n",
    "  j               = j + 1\n",
    "df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HIJ-iSTLoM7Z",
    "nbgrader": {
     "checksum": "aeca382735ff5ba1ffad547bbcd1edfb",
     "grade": false,
     "grade_id": "cell-8f4d3077f3138b00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's see how many different types of \"intent\" are present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "78de28df7ef90b9b419007a17f398009",
     "grade": false,
     "grade_id": "cell-cc82f0f6bb3e64c3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "intent_counts = df_train['intent'].value_counts() # type: ignore\n",
    "intent_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates data extraction from pandas DataFrames, converting structured tabular data into NumPy arrays for machine learning preprocessing. It systematically separates the different components of the ATIS dataset into individual arrays for both training and testing purposes.\n",
    "\n",
    "## Data Extraction Strategy\n",
    "\n",
    "The code extracts three distinct types of information from each DataFrame: queries (the actual user input text), intents (the classification labels indicating what the user wants to accomplish), and slot filling data (the structured entity labels for each word in the query). By using the `.values` attribute on pandas Series, the code converts the DataFrame columns directly into NumPy arrays, which are the preferred data format for most machine learning libraries. This conversion eliminates pandas-specific metadata and index information, leaving only the raw data values.\n",
    "\n",
    "## Training and Testing Data Separation\n",
    "\n",
    "The extraction follows a consistent pattern for both training and testing datasets, creating six separate arrays total. The training arrays (`query_data_train`, `intent_data_train`, `slot_data_train`) contain the data that will be used to train the model, while the testing arrays (`query_data_test`, `intent_data_test`, `slot_data_test`) contain held-out data for evaluating model performance. This separation is crucial for unbiased model evaluation, as the testing data represents unseen examples that the model hasn't learned from.\n",
    "\n",
    "## Preparing for Multi-Task Learning\n",
    "\n",
    "This data structure sets up the foundation for a multi-task natural language understanding system. The queries serve as input features, while both intents and slot filling information can serve as target labels for different prediction tasks. In the context of this assignment, the model will primarily focus on slot filling prediction, but having the intent data available allows for potential expansion into joint intent classification and slot filling tasks. The consistent naming convention and parallel structure make it easy to feed this data into TensorFlow/Keras models later in the pipeline.\n",
    "\n",
    "**Key consideration**: The `.values` extraction assumes that the DataFrames are properly aligned (same number of rows in the same order) between training and testing sets, which is typically guaranteed when the data comes from a properly split dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fc4fbeda487ff04cdc2cc610737911b2",
     "grade": false,
     "grade_id": "cell-73f19e232d0f915f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract \n",
    "#   query_data_train, intent_data_train, slot_data_train,\n",
    "#   query_data_test,  intent_data_test,  slot_data_test \n",
    "# from the train and test dataframes\n",
    "query_data_train  = df_train['query'].values # type: ignore\n",
    "intent_data_train = df_train['intent'].values # type: ignore\n",
    "slot_data_train   = df_train['slot filling'].values # type: ignore\n",
    "query_data_test   = df_test['query'].values # type: ignore\n",
    "intent_data_test  = df_test['intent'].values # type: ignore\n",
    "slot_data_test    = df_test['slot filling'].values # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bfAICLPgQym1",
    "nbgrader": {
     "checksum": "e0fe9de775f659e14dffb4e5854fcbc4",
     "grade": false,
     "grade_id": "cell-aee586687bc114c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We briefly mentioned what the difference were between slot filling and intent in the introduction, but is worth going into more detail.\n",
    "\n",
    "As an example, let’s consider the user query “*i want to fly from boston at 838 am and arrive in denver at 1110 in the morning*”. The model should classify this user query as “**flight**” intent. It should also parse the query, identify and fill all slots necessary for understanding the query. Although the words “I”, “want”, “to”, “fly”, “from”, “at”, “and”, “arrive”, “in”, “the” contribute to understand the context of the intent, the model should correctly label the entities needed to fulfill user’s goal in its intention to take a flight. These are “boston” as departure city (B-fromloc.city), “8:38 am” as departure time (B-depart_time.time), “denver” as destination city (B-toloc.city_name), “11:10” as arrival time (B-arrive_time.time) and “morning” as arrival period of day (B-arrive_time.period_of_day). The 123 slot categories are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dea6fc77ec9cc9c06b87f6d1f0f7f86e",
     "grade": false,
     "grade_id": "cell-53f178f13453634f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "unique_slots = set()\n",
    "for s in slot_data_train: # type: ignore\n",
    "  unique_slots = unique_slots.union(set(s.split()))\n",
    "unique_slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates how to count the number of unique slot categories in the ATIS dataset by determining the size of the `unique_slots` set. It provides a quick way to understand the complexity and scope of the slot filling task.\n",
    "\n",
    "## Counting Unique Elements with len()\n",
    "\n",
    "The `len()` function is a built-in Python function that returns the number of items in a collection. When applied to a set like `unique_slots`, it efficiently counts the number of unique elements without any duplicates. This is particularly useful here because sets automatically eliminate duplicate entries, so `len(unique_slots)` gives us the exact count of distinct slot categories present in the training data. The function works by calling the object's `__len__()` method internally, making it compatible with various Python data structures including lists, tuples, strings, and sets.\n",
    "\n",
    "## Understanding Dataset Complexity\n",
    "\n",
    "In the context of this natural language understanding task, knowing the number of unique slot categories is crucial for understanding the model's classification challenge. The slot filling task requires the model to assign each word in a query to one of these categories (plus non-slot tokens). A higher number of categories indicates a more complex classification problem, as the model needs to distinguish between many different entity types like cities, times, airlines, and airports. This count directly influences model architecture decisions, such as the size of the final output layer in the neural network.\n",
    "\n",
    "## Practical Implications for Model Design\n",
    "\n",
    "The result of `len(unique_slots)` (which shows 123 slot categories in this dataset) reveals that this is a fairly complex multi-class classification problem. This number will be used later in the code to set the `slot_vocab_size` parameter, which determines the output dimension of the model's final classification layer. Understanding this complexity upfront helps in setting appropriate model hyperparameters and managing expectations about training time and potential accuracy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1677215293950,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "Nzi6TzChDYDX",
    "nbgrader": {
     "checksum": "11d0ac8e936b9d950d8548104ea46474",
     "grade": false,
     "grade_id": "cell-e28afa0a2f2d10db",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "3d794f42-8249-4704-cfd5-3a25dbdb1fd8"
   },
   "outputs": [],
   "source": [
    "len(unique_slots) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "khtDWvcGwTeJ",
    "nbgrader": {
     "checksum": "ace784ea61a403d741c8f45bbfccf24f",
     "grade": false,
     "grade_id": "cell-0afa3dd085ac82ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**123 slot categories!!**\n",
    "## Transformers\n",
    "\n",
    "The code below imports the `TransformerEncoder` and `PositionalEmbedding` classes from the `HODL` library, which are essential components for building transformer-based models in natural language processing tasks. These components will be used to create a transformer encoder architecture that can effectively process sequential data like text.\n",
    "\n",
    "### Explain the attention mechanism\n",
    "\n",
    "The attention mechanism is a fundamental component of transformer models that allows the model to focus on different parts of the input sequence when making predictions. Unlike traditional recurrent neural networks (RNNs), which process sequences in order, transformers use self-attention to compute a representation of the entire sequence at once.\n",
    "The self-attention mechanism works by calculating a weighted sum of the input embeddings, where the weights are determined by the relevance of each word to every other word in the sequence. This allows the model to capture long-range dependencies and relationships between words, regardless of their position in the sequence. The attention scores are computed using query, key, and value vectors derived from the input embeddings, enabling the model to dynamically adjust its focus based on the context.\n",
    "\n",
    "### Encoder Model\n",
    "\n",
    "The `TransformerEncoder` class implements the encoder part of the transformer architecture, which consists of multiple layers of self-attention and feed-forward neural networks. Each layer applies self-attention to the input sequence, allowing the model to weigh the importance of different words in the context of each other. This is particularly useful for understanding relationships between words in a sentence, regardless of their position. \n",
    "\n",
    "# Positional Embedding\n",
    "\n",
    "The `PositionalEmbedding` class adds positional information to the input embeddings, which is crucial because it helps the model understand the order of words in the sequence. Unlike RNNs, transformers process all words simultaneously, so they need a way to incorporate word order information. The `PositionalEmbedding` class achieves this by adding sinusoidal position encodings to the input embeddings, allowing the model to differentiate between words based on their position in the sequence.\n",
    "\n",
    "Because the code for transformer encoder architecture is a bit complicated to write, we have decided to package it. This means that you can import it directly from our own \"library\" (in the same way you do it for Keras layers).\n",
    "\n",
    "Import `TransformerEncoder`, and `PositionalEmbedding` from 'HODL' file present in the current file directory. \n",
    "\n",
    "Hint: Take a look at the `HODL.py` file on the left-sidebar menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 1076,
     "status": "error",
     "timestamp": 1677217570584,
     "user": {
      "displayName": "Domingo Genao",
      "userId": "16575500860980135990"
     },
     "user_tz": 300
    },
    "id": "Bms2Np84XkZw",
    "nbgrader": {
     "checksum": "67ad41d91a3209b087b27e973f992356",
     "grade": false,
     "grade_id": "cell-2dd0ad2cc870b1d0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "d5ec2d24-f456-4cbe-f623-a24661b7c412"
   },
   "outputs": [],
   "source": [
    "from HODL import TransformerEncoder, PositionalEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates basic NumPy array slicing to examine the first five elements of the training query data. It's a common exploratory data analysis technique used to quickly inspect the structure and content of your dataset before proceeding with machine learning preprocessing.\n",
    "\n",
    "## Array Slicing for Data Inspection\n",
    "\n",
    "The syntax `query_data_train[:5]` uses Python's slice notation to extract the first five elements from the `query_data_train` array. The colon (`:`) indicates slicing, and the number `5` specifies that we want elements from index 0 up to (but not including) index 5. This is equivalent to writing `query_data_train[0:5]` but more concise. Since `query_data_train` contains the raw text queries from the ATIS dataset, this operation will display the first five user queries as strings.\n",
    "\n",
    "## Understanding Your Input Data\n",
    "\n",
    "This inspection step is crucial for understanding what your model will be working with. By examining these sample queries, you can see the natural language patterns, vocabulary, and sentence structures that your transformer model will need to process. The queries typically contain travel-related requests like flight bookings, airport information, or schedule inquiries. This preview helps verify that the data extraction from the DataFrame was successful and gives you insight into the complexity and variety of the input text.\n",
    "\n",
    "## Best Practice for Data Exploration\n",
    "\n",
    "Displaying a small sample of your data is a fundamental step in any machine learning pipeline. It allows you to catch potential issues early, such as unexpected data formats, encoding problems, or missing values. In the context of this natural language processing task, seeing the actual text helps you understand the linguistic challenges your model will face and can inform decisions about preprocessing steps, tokenization strategies, and model architecture choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1652987923138,
     "user": {
      "displayName": "Vivek Farias",
      "userId": "06849851968315949902"
     },
     "user_tz": 240
    },
    "id": "oWYzkeuVy56d",
    "nbgrader": {
     "checksum": "513d011114b49e44516ea14763ed7ba6",
     "grade": false,
     "grade_id": "cell-03b8d7aacc60e938",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "3139ce21-38fb-47a5-e6d6-de718d2044e8"
   },
   "outputs": [],
   "source": [
    "query_data_train[:5] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates array slicing to inspect the first five elements of the slot filling training data. It serves as a crucial data exploration step to understand the structure and format of the target labels that the model will need to predict.\n",
    "\n",
    "## Examining Slot Filling Labels\n",
    "\n",
    "The syntax `slot_data_train[:5]` uses Python's slice notation to extract the first five slot filling sequences from the training dataset. Unlike the query data which contains natural language text, `slot_data_train` contains structured label sequences that correspond to each word in the queries. These labels follow the BIO (Begin-Inside-Outside) tagging scheme, where each token is assigned a specific slot category like \"B-fromloc.city\" for the beginning of a departure city or \"O\" for tokens that don't belong to any slot.\n",
    "\n",
    "## Understanding Label Structure\n",
    "\n",
    "This inspection reveals the complexity of the slot filling task by showing how each word in a query is systematically labeled with its semantic role. The slot labels provide fine-grained entity recognition, identifying not just that \"Boston\" is a city, but specifically that it's a departure location (\"B-fromloc.city\"). This level of detail allows the model to distinguish between different types of entities that might appear similar but serve different purposes in travel queries, such as departure cities versus destination cities.\n",
    "\n",
    "## Preparing for Model Training\n",
    "\n",
    "By examining these sample slot sequences, you can verify that the data extraction process worked correctly and understand the exact format your model will need to output. This preview also helps you appreciate the alignment between queries and their corresponding slot labels - each position in the slot sequence corresponds to a word in the query. This one-to-one correspondence is essential for training the sequence labeling model and ensures that the transformer can learn to map input words to their appropriate semantic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1652987941147,
     "user": {
      "displayName": "Vivek Farias",
      "userId": "06849851968315949902"
     },
     "user_tz": 240
    },
    "id": "tUiQlh2ty2Qc",
    "nbgrader": {
     "checksum": "8e8e983eb3b0189be33f7df260e27b32",
     "grade": false,
     "grade_id": "cell-5ec1952576374293",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "716c7849-2311-4b63-9186-b436b9d19179"
   },
   "outputs": [],
   "source": [
    "slot_data_train[:5] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates the text preprocessing pipeline using Keras TextVectorization layers to convert raw text data into numerical representations suitable for deep learning models. It establishes two separate vectorization pipelines: one for slot labels (targets) and another for query text (inputs), creating the foundation for training a sequence labeling model.\n",
    "\n",
    "## Setting Up Text Vectorization Parameters\n",
    "\n",
    "The code begins by defining `max_query_length = 30`, which sets a fixed sequence length for all inputs and outputs. This standardization is crucial for batch processing in neural networks, as all sequences in a batch must have the same dimensions. The TextVectorization layers will either truncate longer sequences or pad shorter ones with zeros to reach this exact length. This parameter should be chosen based on the dataset characteristics - too short and you'll lose important information, too long and you'll waste computational resources on padding.\n",
    "\n",
    "## Slot Label Vectorization Pipeline\n",
    "\n",
    "The first TextVectorization layer processes the slot filling labels with `standardize=None`, which is important because slot labels like \"B-fromloc.city\" and \"O\" should remain exactly as they are without any text preprocessing. The `adapt()` method analyzes the training slot data to build a vocabulary mapping each unique slot label to an integer index. This creates a consistent encoding scheme where \"O\" might map to index 1, \"B-fromloc.city\" to index 2, and so on. The `vocabulary_size()` call returns the total number of unique slot categories plus special tokens, which will be used later to define the output layer size of the neural network.\n",
    "\n",
    "## Query Text Vectorization Pipeline\n",
    "\n",
    "The second TextVectorization layer handles the input queries using default standardization, which includes lowercasing, punctuation removal, and whitespace normalization. This preprocessing helps the model generalize better by treating \"Boston\" and \"boston\" as the same token. After adapting to the query training data, this vectorizer can convert any text input into a sequence of integer indices representing words in its learned vocabulary. The resulting `query_vocab_size` determines the size of the embedding layer that will convert these integer indices into dense vector representations.\n",
    "\n",
    "## Creating Numerical Datasets\n",
    "\n",
    "The final step applies both vectorizers to transform the raw text data into numerical tensors. The slot vectorizer converts `slot_data_train` and `slot_data_test` into `target_train` and `target_test` - integer sequences where each position corresponds to the slot label for that word position. Similarly, the query vectorizer transforms the text queries into `source_train` and `source_test` - integer sequences representing the input words. These numerical representations are now ready to be fed into the transformer model for training and evaluation.\n",
    "\n",
    "**Key consideration**: The order of operations matters here - you must call `adapt()` on the training data before applying the vectorizer to both training and test sets to ensure consistent vocabulary mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ma9COj0675XG",
    "nbgrader": {
     "checksum": "b7534ff5bb3207a91a5562e799caf96a",
     "grade": false,
     "grade_id": "cell-3e4cec4d2c2d3fcd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "max_query_length = 30\n",
    "# Textvec of slots\n",
    "text_vectorization_slots = keras.layers.TextVectorization( # type: ignore\n",
    "    output_sequence_length = max_query_length,\n",
    "    standardize            = None\n",
    ")\n",
    "text_vectorization_slots.adapt(slot_data_train) # type: ignore\n",
    "#Number of slots\n",
    "slot_vocab_size = text_vectorization_slots.vocabulary_size() \n",
    "target_train    = text_vectorization_slots(slot_data_train) # type: ignore\n",
    "target_test     = text_vectorization_slots(slot_data_test) # type: ignore\n",
    "# Textvec of query\n",
    "text_vectorization_query = keras.layers.TextVectorization( # type: ignore\n",
    "    output_sequence_length = max_query_length\n",
    ")\n",
    "text_vectorization_query.adapt(query_data_train) # type: ignore\n",
    "#Numbr of unique query words\n",
    "query_vocab_size = text_vectorization_query.vocabulary_size()\n",
    "source_train     = text_vectorization_query(query_data_train) # type: ignore\n",
    "source_test      = text_vectorization_query(query_data_test) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates the construction of a transformer-based neural network architecture for slot filling, combining modern attention mechanisms with traditional dense layers to create a sophisticated sequence labeling model. The architecture follows a clear pipeline from input processing through transformer encoding to final classification.\n",
    "\n",
    "## Model Architecture and Hyperparameters\n",
    "\n",
    "The code begins by defining key hyperparameters that control the model's capacity and behavior. The `embedding_dim = 512` sets a relatively large embedding space that allows the model to capture rich semantic representations of words. The `encoder_units = 64` parameter controls the size of the feed-forward network within the transformer encoder, while `units = 128` defines the hidden layer size in the final classifier. The `num_heads = 5` specifies the number of attention heads in the multi-head attention mechanism, allowing the model to focus on different aspects of the input sequence simultaneously.\n",
    "\n",
    "## Input Processing and Positional Embedding\n",
    "\n",
    "The model starts with a `keras.Input` layer that accepts sequences of length `max_query_length`, representing tokenized query text as integer indices. The `PositionalEmbedding` layer then performs two crucial transformations: it converts the integer tokens into dense vector representations using token embeddings, and adds positional information so the model understands word order. This custom layer combines both token and position embeddings by simply adding them together, creating rich input representations that encode both semantic meaning and sequential structure essential for understanding natural language.\n",
    "\n",
    "## Transformer Encoder Processing\n",
    "\n",
    "The heart of the model is the `TransformerEncoder` layer, which applies the self-attention mechanism that has revolutionized natural language processing. This layer allows each word in the sequence to attend to all other words, capturing complex dependencies and relationships regardless of distance. The transformer encoder uses residual connections and layer normalization to ensure stable training, while the multi-head attention mechanism enables the model to focus on different types of relationships simultaneously. The output `encoder_out` contains contextualized representations where each word's embedding has been enriched with information from the entire sequence.\n",
    "\n",
    "## Classification Head and Output\n",
    "\n",
    "The final portion of the architecture consists of a classification head that transforms the transformer's contextualized representations into slot predictions. The first `Dense` layer with ReLU activation serves as a feature transformation layer, followed by `Dropout(0.5)` for regularization to prevent overfitting. The final `Dense` layer with softmax activation produces probability distributions over the `slot_vocab_size` categories for each word position, enabling the model to predict slot labels like \"B-fromloc.city\" or \"O\" for each token in the input sequence.\n",
    "\n",
    "**Key architectural insight**: This design effectively combines the global context modeling capabilities of transformers with task-specific classification layers, creating a model that can understand both local word meanings and global sentence structure while making precise slot filling predictions at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1652988471828,
     "user": {
      "displayName": "Vivek Farias",
      "userId": "06849851968315949902"
     },
     "user_tz": 240
    },
    "id": "w5lT0qaD7wB1",
    "nbgrader": {
     "checksum": "42d43bd4f89205be29929be8f3f37325",
     "grade": false,
     "grade_id": "cell-d533ff53322b2204",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "90403c71-e41a-4a43-e76c-f569ff40d889"
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "embedding_dim = 512\n",
    "encoder_units = 64\n",
    "units         = 128\n",
    "num_heads     = 5\n",
    "# Embedding and Masking\n",
    "inputs        = keras.Input(shape=(max_query_length,)) # type: ignore\n",
    "embedding     = PositionalEmbedding(max_query_length, query_vocab_size, embedding_dim) # type: ignore\n",
    "x             = embedding(inputs)\n",
    "# Transformer Encoding\n",
    "encoder_out   = TransformerEncoder(embedding_dim, encoder_units, num_heads)(x) # type: ignore\n",
    "# Classifier\n",
    "x             = keras.layers.Dense(units, activation='relu')(encoder_out) # type: ignore\n",
    "x             = keras.layers.Dropout(0.5)(x) # type: ignore\n",
    "outputs       = keras.layers.Dense(slot_vocab_size, activation=\"softmax\")(x) # type: ignore\n",
    "model         = keras.Model(inputs, outputs) # type: ignore\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the model compilation step in Keras, which configures the transformer model for training by specifying the optimization algorithm, loss function, and evaluation metrics. Model compilation is a crucial step that bridges the gap between model architecture definition and actual training.\n",
    "\n",
    "## Optimizer Selection and Adam Algorithm\n",
    "\n",
    "The code uses `optimizer=\"adam\"`, which specifies the Adam optimization algorithm for updating the model's weights during training. Adam is an adaptive learning rate optimizer that combines the benefits of two other popular optimizers: AdaGrad (which adapts learning rates based on historical gradients) and RMSprop (which uses a moving average of squared gradients). Adam is particularly well-suited for transformer models because it handles sparse gradients effectively and automatically adjusts learning rates for each parameter individually, making it robust across different types of neural network architectures and datasets.\n",
    "\n",
    "## Loss Function for Multi-Class Classification\n",
    "\n",
    "The `loss=\"sparse_categorical_crossentropy\"` parameter defines how the model measures the difference between predicted and actual slot labels during training. This loss function is specifically designed for multi-class classification problems where each sample belongs to exactly one category out of many possible categories (in this case, the 123+ slot categories). The \"sparse\" variant is used because the target labels are provided as integer indices rather than one-hot encoded vectors, which is more memory-efficient for problems with large vocabularies like slot filling tasks.\n",
    "\n",
    "## Performance Monitoring with Metrics\n",
    "\n",
    "The [`metrics=[\"sparse_categorical_accuracy\"]`](/Users/eneas/Desktop/MIT/WEEK-23-MODULE-21/Portfolio-Assignment-21-1/Assignment21.ipynb) parameter specifies that the model should track accuracy during training and validation. This metric calculates the percentage of predictions where the model correctly identifies the slot category for each token position. Using sparse categorical accuracy is consistent with the sparse categorical crossentropy loss function and provides an intuitive measure of model performance that's easy to interpret - a value of 0.95 means the model correctly predicts 95% of slot labels.\n",
    "\n",
    "**Key consideration**: The compilation step doesn't change the model's architecture but prepares it for training by defining the mathematical framework for learning. These choices (Adam optimizer, sparse categorical crossentropy loss, and accuracy metric) are well-established best practices for sequence labeling tasks in natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "fuDpwFcb8Diz",
    "nbgrader": {
     "checksum": "c3f0543c6b5f984468895a2342ff8ab3",
     "grade": false,
     "grade_id": "cell-6dda9bc3e6ac9ca9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile your model\n",
    "with tf.device(':GPU:0'): # type: ignore\n",
    "    print('Using GPU for training')\n",
    "    model.compile(optimizer=\"adam\", # type: ignore\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates the model training phase in Keras, where the transformer model begins learning to perform slot filling by processing the ATIS dataset. The training process involves setting key hyperparameters and calling the `fit()` method to start the iterative learning process.\n",
    "\n",
    "## Training Hyperparameter Configuration\n",
    "\n",
    "The code begins by defining two critical training hyperparameters. The `BATCH_SIZE = 128` parameter determines how many training examples the model processes simultaneously before updating its weights. A batch size of 128 strikes a good balance between computational efficiency and gradient stability - larger batches provide more stable gradient estimates but require more memory, while smaller batches update weights more frequently but with noisier gradients. The `epochs = 10` parameter specifies that the model will see the entire training dataset 10 times during training, allowing it to gradually learn the patterns in the data through repeated exposure.\n",
    "\n",
    "## Model Training Execution\n",
    "\n",
    "The `model.fit()` method is the core function that orchestrates the training process. It takes `source_train` (the vectorized query text) as input features and `target_train` (the vectorized slot labels) as target outputs, establishing the supervised learning relationship between questions and their corresponding slot annotations. During each epoch, the model processes the training data in batches of 128 examples, computing predictions, calculating loss using the sparse categorical crossentropy function defined during compilation, and updating weights using the Adam optimizer to minimize prediction errors.\n",
    "\n",
    "## Training History and Monitoring\n",
    "\n",
    "The function returns a `history` object that contains detailed logs of the training process, including loss values and accuracy metrics for each epoch. This history is invaluable for understanding how the model's performance evolves during training - you can observe whether the loss is decreasing consistently, whether accuracy is improving, and whether the model might be overfitting or underfitting. The training process will show real-time updates of these metrics, allowing you to monitor the transformer's progress as it learns to map natural language queries to their corresponding slot filling annotations.\n",
    "\n",
    "**Key insight**: This training configuration represents the model's first exposure to the slot filling task, where 10 epochs should provide sufficient learning iterations for the transformer to capture the relationship between travel-related queries and their semantic slot structures.\n",
    "\n",
    "The output `Epoch 1/10` indicates that the model training has just begun and is currently processing the first epoch out of the total 10 epochs specified in the training configuration.\n",
    "\n",
    "## Understanding Epoch Output\n",
    "\n",
    "An epoch represents one complete pass through the entire training dataset. When you see `Epoch 1/10`, it means the model is starting to process all the training examples for the first time. During this epoch, the transformer model will:\n",
    "\n",
    "1. **Process all training batches**: The model will go through the `source_train` data (vectorized queries) in batches of 128 examples each\n",
    "2. **Make predictions**: For each batch, the model will predict slot labels for every word position\n",
    "3. **Calculate loss**: Using sparse categorical crossentropy to measure prediction accuracy\n",
    "4. **Update weights**: The Adam optimizer will adjust model parameters to minimize prediction errors\n",
    "\n",
    "## What Happens During Training\n",
    "\n",
    "As the epoch progresses, you would typically see additional output showing:\n",
    "- Current batch number and total batches\n",
    "- Real-time loss values decreasing as the model learns\n",
    "- Accuracy metrics improving as predictions become more accurate\n",
    "- Time estimates for completion\n",
    "\n",
    "## Training Progress Monitoring\n",
    "\n",
    "The fact that training has started successfully indicates that:\n",
    "- The model architecture is correctly defined\n",
    "- Input and target data shapes are compatible\n",
    "- The compilation settings (optimizer, loss function, metrics) are working properly\n",
    "- The transformer is ready to learn the relationship between travel queries and their corresponding slot filling annotations\n",
    "\n",
    "**Expected behavior**: After this first epoch completes, you'll see the final loss and accuracy values for epoch 1, then the training will continue through epochs 2-10, with the model gradually improving its slot filling performance with each pass through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 24507,
     "status": "ok",
     "timestamp": 1652988522353,
     "user": {
      "displayName": "Vivek Farias",
      "userId": "06849851968315949902"
     },
     "user_tz": 240
    },
    "id": "XtC7kzdDdMqG",
    "nbgrader": {
     "checksum": "7f5c3be91c824600dbba65bf43128871",
     "grade": false,
     "grade_id": "cell-0ce7e14d9abad00b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "4c7bd627-7165-44d0-f11a-1bcfa9f35af6"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "epochs     = 10\n",
    "tf.config.set_soft_device_placement(True) # type: ignore\n",
    "with tf.device(':GPU:0'): # type: ignore\n",
    "    print('Using GPU for training')\n",
    "    # Train the model\n",
    "    history = model.fit(source_train,  # type: ignore\n",
    "                    target_train, # type: ignore\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    epochs     = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates a comprehensive evaluation pipeline for the slot filling model, including a custom accuracy calculation function and model prediction on test data. The evaluation provides two different accuracy metrics to understand both overall performance and slot-specific performance.\n",
    "\n",
    "## Custom Accuracy Function Design\n",
    "\n",
    "The `slot_filling_accuracy` function implements a sophisticated accuracy calculation that handles the unique challenges of sequence labeling tasks. The `not_padding` mask uses `np.not_equal(actual, 0)` to exclude padding tokens (represented by zeros) from accuracy calculations, ensuring that only meaningful positions contribute to the final metric. The function offers two evaluation modes through the `only_slots` parameter: when `False`, it calculates accuracy across all tokens including non-slot \"O\" labels; when `True`, it focuses exclusively on actual slot entities by filtering out the \"O\" tokens using `text_vectorization_slots(['O']).numpy()[0, 0]` to get the integer representation of the non-slot token.\n",
    "\n",
    "## Prediction Generation and Device Management\n",
    "\n",
    "The code uses `tf.config.set_soft_device_placement(True)` to enable flexible device placement, allowing TensorFlow to automatically choose the best available device for operations. The `with tf.device('/CPU:0'):` context manager forces prediction to run on CPU, which can be useful for ensuring consistent behavior or avoiding GPU memory issues during evaluation. The prediction process involves calling `model.predict(source_test)` to get probability distributions for each token position, then using `np.argmax(axis=-1)` to convert these probabilities into predicted class indices. The `.reshape(-1)` operation flattens both predicted and actual arrays into 1D vectors for easier comparison.\n",
    "\n",
    "## Dual Accuracy Metrics and Performance Insights\n",
    "\n",
    "The evaluation calculates two distinct accuracy metrics that provide complementary insights into model performance. The overall accuracy (`only_slots=False`) measures how well the model performs across all token positions, including the common \"O\" (outside) labels that represent non-slot tokens. The slot-specific accuracy (`only_slots=True`) focuses exclusively on the model's ability to correctly identify and classify actual slot entities, providing a more challenging metric that excludes the easier-to-predict non-slot tokens. This dual approach is crucial for understanding slot filling performance because models might achieve high overall accuracy by correctly predicting many \"O\" tokens while still struggling with the more complex slot classification task.\n",
    "\n",
    "**Key insight**: The custom accuracy function's use of `np.dot(correct_predictions, weights) / sample_length` is mathematically equivalent to a simple mean calculation but demonstrates a more general framework that could easily be extended to support weighted accuracy metrics if certain token types were considered more important than others in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1652988529810,
     "user": {
      "displayName": "Vivek Farias",
      "userId": "06849851968315949902"
     },
     "user_tz": 240
    },
    "id": "epn7rEY2Qgih",
    "nbgrader": {
     "checksum": "20aba08ab307a680ff81005006afc12d",
     "grade": false,
     "grade_id": "cell-9b2a7e3c1bc3fde7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "c671370b-95f8-4ca7-ef1a-3aee05b81203"
   },
   "outputs": [],
   "source": [
    "# numpy is already imported in a previous cell, so no need to re-import.\n",
    "\n",
    "\n",
    "def slot_filling_accuracy(actual, predicted, only_slots=False):\n",
    "  not_padding = np.not_equal(actual, 0) # type: ignore #+ np.not_equal(predicted, 0)\n",
    "  if only_slots:\n",
    "    non_slot_token      = text_vectorization_slots(['O']).numpy()[0, 0] # type: ignore\n",
    "    slots               = np.not_equal(actual, non_slot_token) # type: ignore\n",
    "    correct_predictions = np.equal(actual, predicted)[not_padding * slots] # type: ignore\n",
    "  else:\n",
    "    correct_predictions = np.equal(actual, predicted)[not_padding] # type: ignore\n",
    "  sample_length = len(correct_predictions)\n",
    "  weights       = np.ones(sample_length) # type: ignore\n",
    "  return np.dot(correct_predictions, weights) / sample_length # type: ignore\n",
    "tf.config.set_soft_device_placement(True) # type: ignore\n",
    "with tf.device(':GPU:0'): # type: ignore\n",
    "  print('Using GPU for inference' )\n",
    "  predicted = np.argmax(model.predict(source_test), axis=-1).reshape(-1) # type: ignore\n",
    "actual    = target_test.numpy().reshape(-1) # type: ignore\n",
    "acc       = slot_filling_accuracy(actual, predicted, only_slots=False)\n",
    "acc_slots = slot_filling_accuracy(actual, predicted, only_slots=True)\n",
    "print(f'Accuracy = {acc:.3f}')\n",
    "print(f'Accuracy on slots = {acc_slots:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RbKQV0LGhI__",
    "nbgrader": {
     "checksum": "06fbf6505e2b12475c74848f15ea5d34",
     "grade": false,
     "grade_id": "cell-952c3b22f89bebce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we get 92% accuracy on the slots and 97% accuracy in general. This is so much better!!\n",
    "\n",
    "Let's see some examples:\n",
    "\n",
    "\n",
    "This code demonstrates a practical inference pipeline that showcases the trained transformer model's slot filling capabilities on real-world travel queries. It combines prediction generation with human-readable output formatting to create an interactive demonstration of the model's performance.\n",
    "\n",
    "## Prediction Function Architecture\n",
    "\n",
    "The `predict_slots_query` function encapsulates the complete inference workflow for converting raw text queries into slot predictions. The function begins by applying the same text vectorization process used during training through `text_vectorization_query([query])`, which converts the input string into a numerical sequence that matches the model's expected input format. The model then generates probability distributions for each token position via `model.predict(sentence)`, and `np.argmax(axis=-1)[0]` converts these probabilities into the most likely slot category indices for each word position in the sequence.\n",
    "\n",
    "## Vocabulary Inversion and Decoding\n",
    "\n",
    "A crucial component of the function is the creation of an inverse vocabulary mapping using `dict(enumerate(text_vectorization_slots.get_vocabulary()))`. This creates a dictionary that maps integer indices back to their corresponding slot labels, essentially reversing the encoding process performed during preprocessing. The `enumerate` function pairs each vocabulary word with its index position, creating key-value pairs like `{0: '', 1: 'O', 2: 'B-fromloc.city'}`. The final step uses this inverse mapping to decode the numerical predictions back into human-readable slot labels, joining them with spaces to create a formatted output string that aligns with the original query structure.\n",
    "\n",
    "## Comprehensive Testing Examples\n",
    "\n",
    "The examples list provides a diverse set of test cases that demonstrate different aspects of the model's slot filling capabilities. Simple directional phrases like \"from los angeles\" and \"to los angeles\" test basic location recognition, while complex queries like \"cheapest flight from boston to los angeles tomorrow\" challenge the model's ability to handle multiple entities and modifiers. The inclusion of information-seeking queries such as \"what is the airport at orlando\" tests the model's understanding of different intent patterns, and the variation between \"flight from boston to santiago\" and \"flight boston to santiago\" evaluates robustness to different grammatical constructions.\n",
    "\n",
    "**Key insight**: This demonstration effectively bridges the gap between technical model output and practical application, showing how the transformer's numerical predictions translate into meaningful semantic annotations that could power real travel booking systems or conversational AI interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1652903886155,
     "user": {
      "displayName": "Vivek Farias",
      "userId": "06849851968315949902"
     },
     "user_tz": 300
    },
    "id": "WuWqtUjXP6FM",
    "nbgrader": {
     "checksum": "949d550b6ac66d6db3a8140caf4abc80",
     "grade": false,
     "grade_id": "cell-e0cbcee42c1da9ca",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "2d072f83-3fe6-4554-c8eb-3b9e8a83c312"
   },
   "outputs": [],
   "source": [
    "# Define the predict_slots_query() function which takes in a query\n",
    "def predict_slots_query(query):\n",
    "  sentence           = text_vectorization_query([query]) # type: ignore\n",
    "  prediction         = np.argmax(model.predict(sentence), axis=-1)[0] # type: ignore\n",
    "  inverse_vocab      = dict(enumerate(text_vectorization_slots.get_vocabulary())) # type: ignore\n",
    "  decoded_prediction = \" \".join(inverse_vocab[int(i)] for i in prediction)\n",
    "  return decoded_prediction\n",
    "examples = [\n",
    "            'from los angeles',\n",
    "            'to los angeles',\n",
    "            'from boston',\n",
    "            'to boston',\n",
    "            'cheapest flight from boston to los angeles tomorrow',\n",
    "            'what is the airport at orlando',\n",
    "            'what are the air restrictions on flights from pittsburgh to atlanta for the airfare of 416 dollars',\n",
    "            'flight from boston to santiago',\n",
    "            'flight boston to santiago'\n",
    "]\n",
    "for e in examples:\n",
    "    print(e)\n",
    "    print(predict_slots_query(e))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XTN8GEychT4o",
    "nbgrader": {
     "checksum": "4068bdcff602a94b234350f1be566797",
     "grade": false,
     "grade_id": "cell-83816d28128c7473",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Even though 'Santiago' is not a city that is present in the training data set, it is still capable of recognizing it as a destination city name just by context! This is the power of the attention mechanism of transformers.\n",
    "\n",
    "Can we get even better accuracy if we train for longer? Let's try!\n",
    "\n",
    "\n",
    "This code demonstrates extended training of the transformer model, where the learning process is continued for an additional 20 epochs to explore whether longer training can improve slot filling performance. This represents a common experimental approach in deep learning where researchers investigate the relationship between training duration and model accuracy.\n",
    "\n",
    "## Extended Training Configuration\n",
    "\n",
    "The code sets `epochs = 20`, which means the model will process the entire training dataset 20 more times beyond the initial 10 epochs completed earlier. This extended training period allows the model to continue refining its understanding of the slot filling task through additional exposure to the training examples. The decision to train for 20 additional epochs represents a hypothesis that the model hasn't yet reached its optimal performance and can benefit from more learning iterations. The same `BATCH_SIZE` from the previous training session is maintained, ensuring consistency in the training process and allowing for direct comparison of results.\n",
    "\n",
    "## Continued Learning Process\n",
    "\n",
    "The `model.fit()` call resumes training from the current state of the model weights, rather than starting from scratch. This continuation approach is valuable because it builds upon the knowledge already acquired during the initial 10 epochs, allowing the transformer to make incremental improvements to its slot classification abilities. During these additional epochs, the Adam optimizer will continue adjusting the model parameters based on the sparse categorical crossentropy loss, potentially discovering more nuanced patterns in the relationship between travel queries and their corresponding slot annotations.\n",
    "\n",
    "## Performance Optimization Strategy\n",
    "\n",
    "This extended training approach reflects a fundamental trade-off in machine learning between training time and model performance. While longer training can lead to better accuracy, it also increases the risk of overfitting, where the model becomes too specialized to the training data and performs poorly on new, unseen examples. The `history` object returned from this extended training will contain valuable information about whether the additional epochs resulted in continued improvement or if the model has reached a performance plateau, helping to inform decisions about optimal training duration for this slot filling task.\n",
    "\n",
    "**Key consideration**: Extended training beyond the initial epochs allows experimentation with the training duration to find the sweet spot between underfitting (too little training) and overfitting (too much training), ultimately seeking to maximize the model's ability to generalize to new travel-related queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 33695,
     "status": "ok",
     "timestamp": 1649460922490,
     "user": {
      "displayName": "Patricio Foncea Araneda",
      "userId": "06631388810550008805"
     },
     "user_tz": 240
    },
    "id": "vNYEbhK5Q9pR",
    "nbgrader": {
     "checksum": "be9b7668382ead4b685a05a6d95f50f9",
     "grade": false,
     "grade_id": "cell-7c252040d4b941fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "a2fed8f4-4112-41cb-e6f8-268c854aa73b"
   },
   "outputs": [],
   "source": [
    "epochs  = 20\n",
    "history = model.fit(source_train,  # type: ignore\n",
    "                    target_train, # type: ignore\n",
    "                    batch_size = BATCH_SIZE, # type: ignore\n",
    "                    epochs     = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates a comprehensive evaluation pipeline for the slot filling model after extended training, featuring an updated custom accuracy function and dual-metric assessment to measure the transformer's improved performance on the test dataset.\n",
    "\n",
    "## Simplified Accuracy Function Implementation\n",
    "\n",
    "The `slot_filling_accuracy` function has been streamlined compared to the earlier version, replacing the previous `np.dot(correct_predictions, weights) / sample_length` calculation with the more direct `np.sum(correct_predictions) / sample_length`. This change maintains identical functionality while improving code readability and eliminating the unnecessary creation of a weights array filled with ones. The function still implements the same sophisticated logic for handling padding tokens through the `not_padding` mask and provides the dual evaluation modes via the `only_slots` parameter, but now uses the more intuitive sum-and-divide approach for calculating the mean accuracy.\n",
    "\n",
    "## Model Prediction and Post-Processing Pipeline\n",
    "\n",
    "The prediction generation follows the same robust pipeline as before, with `tf.device('/CPU:0')` ensuring consistent execution on CPU for reliable evaluation results. The `model.predict(source_test)` call generates probability distributions for each token position, which are then converted to class predictions using `np.argmax(axis=-1)`. The subsequent `.reshape(-1)` operations flatten both the predicted and actual label arrays into one-dimensional vectors, creating aligned sequences that can be directly compared element-by-element for accuracy calculation.\n",
    "\n",
    "## Performance Assessment After Extended Training\n",
    "\n",
    "The evaluation computes both overall accuracy and slot-specific accuracy metrics to provide a comprehensive view of how the additional 20 epochs of training affected model performance. The overall accuracy includes all token predictions (including the common \"O\" non-slot labels), while the slot-specific accuracy focuses exclusively on the model's ability to correctly classify actual slot entities. This dual assessment is particularly valuable after extended training because it reveals whether the additional epochs improved the model's ability to handle the more challenging slot classification task, or if gains were primarily in the easier non-slot token predictions.\n",
    "\n",
    "**Key insight**: The simplified accuracy calculation using `np.sum()` directly demonstrates that in NumPy, when working with boolean arrays, summing gives the count of `True` values, making this approach both more readable and computationally efficient than the previous dot product implementation while maintaining mathematical equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1649460927520,
     "user": {
      "displayName": "Patricio Foncea Araneda",
      "userId": "06631388810550008805"
     },
     "user_tz": 240
    },
    "id": "afcttyPQe1U8",
    "nbgrader": {
     "checksum": "7f28c48f8c0715b86fcda17518c7b5e2",
     "grade": false,
     "grade_id": "cell-f037dbc7269abfb2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "52bf8087-e589-4542-a844-bacc82b49205"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def slot_filling_accuracy(actual, predicted, only_slots=False):\n",
    "  not_padding = np.not_equal(actual, 0) # type: ignore\n",
    "  if only_slots:\n",
    "    non_slot_token      = text_vectorization_slots(['O']).numpy()[0, 0] # type: ignore\n",
    "    slots               = np.not_equal(actual, non_slot_token) # type: ignore\n",
    "    correct_predictions = np.equal(actual, predicted)[not_padding * slots] # type: ignore\n",
    "  else:\n",
    "    correct_predictions = np.equal(actual, predicted)[not_padding]\n",
    "  sample_length = len(correct_predictions)\n",
    "  return np.sum(correct_predictions) / sample_length\n",
    "with tf.device('/CPU:0'): # type: ignore\n",
    "  predicted = np.argmax(model.predict(source_test), axis=-1).reshape(-1) # type: ignore\n",
    "actual    = target_test.numpy().reshape(-1) # type: ignore\n",
    "acc       = slot_filling_accuracy(actual, predicted, only_slots=False)\n",
    "acc_slots = slot_filling_accuracy(actual, predicted, only_slots=True)\n",
    "print(f'Accuracy          = {acc:.3f}')\n",
    "print(f'Accuracy on slots = {acc_slots:.3f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "XmxYrFl9m1hM",
    "IzNTx1rf--xQ",
    "a-PxWKVAZ7Pt",
    "TAoHWpgsn5oX"
   ],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1JVS0Wu5j_g43BRC1bihmaKbbkLWHsURA",
     "timestamp": 1652901024714
    },
    {
     "file_id": "1cxBwFsYmUxf0R0uv8vFkTH0_G1O4BOP8",
     "timestamp": 1649854989737
    },
    {
     "file_id": "18wniNWOOgh47kT-rppsOzQRLop0erp-x",
     "timestamp": 1649780916455
    }
   ]
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

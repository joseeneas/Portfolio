{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "### ```Portfolio-Assignment-20-1```\n",
    "\n",
    "This notebook demonstrates how to compile a model using TensorFlow and Keras.\n",
    "\n",
    "This code snippet loads the CIFAR-100 dataset, initializes a ResNet50 model, compiles it with the Adam optimizer and sparse categorical crossentropy loss, and trains it for 5 epochs.\n",
    "```python\n",
    "import tensorflow as tf\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=100,\n",
    "    pooling='avg',\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "This code imports several essential libraries for data analysis and machine learning in Python. `pandas` is imported as `pd` for data manipulation and handling tabular data. `tensorflow` is imported as `tf`, and its `keras` module is also imported, both of which are used for building and training deep learning models. The `train_test_split` function from `sklearn.model_selection` is included to split datasets into training and testing sets, which is a common step in machine learning workflows. Finally, `tf.random.set_seed(42)` sets the random seed for TensorFlow, ensuring reproducibility of results by making random operations deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jjqUzwe969mu",
    "nbgrader": {
     "checksum": "62f7819e52083a254c81c2228bc31591",
     "grade": false,
     "grade_id": "cell-d5c13075b6763d8d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas                      as pd\n",
    "import tensorflow                  as tf                # type: ignore\n",
    "from   tensorflow              import keras             # type: ignore\n",
    "from   tensorflow.keras.layers import TextVectorization # type: ignore\n",
    "from   sklearn.model_selection import train_test_split  # type: ignore\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OI7tGV857bn0",
    "nbgrader": {
     "checksum": "3f83776f4e0ac502b9e769d736f6c41d",
     "grade": false,
     "grade_id": "cell-c794bfd6201853f8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "IMDB movie reviews\n",
    "\n",
    "Retrieving and preparing the Data\n",
    "\n",
    "We will work with the IMDb movie reviews data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8cf131872ebd7d8399460943470bbe2",
     "grade": false,
     "grade_id": "cell-20a04a21a94fc74e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# question 1\n",
    "# Read in the IMDB Dataset into \"data\". Do not set an index column\n",
    "data = pd.read_csv('files/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "The code `data.head()` displays the first five rows of the DataFrame `data`. This is a common practice in data analysis to quickly inspect the initial entries of a dataset. By viewing these rows, you can verify that the data has been loaded correctly, check the column names, and get an initial sense of the structure and contents of the DataFrame before proceeding with further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ab187fcf006207c13e1220389ed1fb4",
     "grade": false,
     "grade_id": "cell-106c15bf7fc4e150",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "00d0d4c2-ec5c-410d-a8bf-e83d97074848",
       "rows": [
        [
         "0",
         "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.",
         "positive"
        ],
        [
         "1",
         "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.",
         "positive"
        ],
        [
         "2",
         "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.",
         "positive"
        ],
        [
         "3",
         "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.",
         "negative"
        ],
        [
         "4",
         "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.",
         "positive"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code transforms the `'sentiment'` column in the DataFrame `data` from text labels to numeric values. It uses the `apply` method with a lambda function to convert each entry: if the sentiment is `'positive'`, it assigns a value of `1`; otherwise, it assigns `0`. This process is known as binary encoding and is commonly used to prepare categorical data for machine learning models, which typically require numeric input. By converting `'positive'` and `'negative'` sentiments to `1` and `0`, the data becomes suitable for classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "72caedc853058ef311a7d160aa7f0b9a",
     "grade": false,
     "grade_id": "cell-c5da00bb8d244d49",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# question 2\n",
    "# Replace all \"negative\" and \"positive\" sentiment values with 0 and 1 respectively.\n",
    "# You can use a simple logical operator instead of label encoding.\n",
    "data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bf7d4c869d3b0584629fbf89a1aa00d2",
     "grade": false,
     "grade_id": "cell-fb60c3ad8cffa7d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "18924215-b804-4020-9be6-6138106988b9",
       "rows": [
        [
         "0",
         "1"
        ],
        [
         "1",
         "1"
        ],
        [
         "2",
         "1"
        ],
        [
         "3",
         "0"
        ],
        [
         "4",
         "1"
        ],
        [
         "5",
         "1"
        ],
        [
         "6",
         "1"
        ],
        [
         "7",
         "0"
        ],
        [
         "8",
         "0"
        ],
        [
         "9",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    0\n",
       "8    0\n",
       "9    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 3\n",
    "# Get the dependent data and assign to y\n",
    "y = data['sentiment']   \n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code splits the dataset into training and testing sets using the `train_test_split` function from scikit-learn. It takes the `'review'` column from the DataFrame `data` as the feature set and `y` as the target variable. The `test_size=0.2` argument specifies that 20% of the data should be reserved for testing, while the remaining 80% is used for training. The `random_state=42` parameter ensures that the split is reproducible by setting a fixed seed for the random number generator. The result is four variables: `X_train` and `y_train` for training, and `X_test` and `y_test` for testing, which are commonly used in machine learning workflows to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0edcaeb4d67e532613270d5df55b4acf",
     "grade": false,
     "grade_id": "cell-841ced4ea0dcaf2f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# question 4\n",
    "# Split the X data (data['review']) and y data into X_train, X_test, y_train, and y_test\n",
    "# With a test size of 0.2 and a random_state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code prints the number of samples in the training and testing datasets. It uses an f-string to format the output, displaying the counts of `X_train` and `X_test` by accessing their `.shape[0]` attributes, which represent the number of rows (samples) in each set. This is useful for quickly verifying the sizes of your splits after using `train_test_split`, ensuring that the data was divided as expected and that you have the correct number of samples for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4RK7K2ZCqTb4",
    "nbgrader": {
     "checksum": "e3ab5d9602a766b48cce4d38187f6322",
     "grade": false,
     "grade_id": "cell-2744a959d2f8f369",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train samples: 40000\n",
      "Test samples : 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Train samples: {X_train.shape[0]}\n",
    "Test samples : {X_test.shape[0]}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "The code `y_train` displays the contents of the variable `y_train`, which represents the target values for the training dataset. In this context, `y_train` contains the sentiment labels (such as 0 for negative and 1 for positive) corresponding to each review in the training set. Viewing `y_train` allows you to inspect the distribution and encoding of the target variable, which is useful for verifying that the data preparation steps have been performed correctly before training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "deletable": false,
    "editable": false,
    "id": "wZtI-YRqTRrg",
    "nbgrader": {
     "checksum": "712aac7799944741784d78205b4c4d50",
     "grade": false,
     "grade_id": "cell-8b669b1b85e4f6aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "01963e47-907e-4ed2-a17d-2299070d3729"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8f43da48-16fa-4cf7-ac98-c2071b41618f",
       "rows": [
        [
         "39087",
         "0"
        ],
        [
         "30893",
         "0"
        ],
        [
         "45278",
         "1"
        ],
        [
         "16398",
         "0"
        ],
        [
         "13653",
         "0"
        ],
        [
         "13748",
         "0"
        ],
        [
         "23965",
         "0"
        ],
        [
         "45552",
         "0"
        ],
        [
         "30219",
         "1"
        ],
        [
         "24079",
         "0"
        ],
        [
         "12181",
         "0"
        ],
        [
         "21848",
         "0"
        ],
        [
         "14290",
         "1"
        ],
        [
         "24755",
         "1"
        ],
        [
         "35013",
         "0"
        ],
        [
         "15361",
         "0"
        ],
        [
         "48038",
         "1"
        ],
        [
         "36311",
         "0"
        ],
        [
         "30239",
         "1"
        ],
        [
         "7195",
         "1"
        ],
        [
         "3579",
         "1"
        ],
        [
         "36147",
         "0"
        ],
        [
         "34095",
         "1"
        ],
        [
         "10441",
         "1"
        ],
        [
         "16307",
         "0"
        ],
        [
         "4204",
         "0"
        ],
        [
         "6957",
         "0"
        ],
        [
         "9163",
         "0"
        ],
        [
         "20627",
         "1"
        ],
        [
         "17567",
         "0"
        ],
        [
         "14832",
         "0"
        ],
        [
         "27029",
         "1"
        ],
        [
         "44197",
         "0"
        ],
        [
         "1",
         "1"
        ],
        [
         "35449",
         "0"
        ],
        [
         "30903",
         "1"
        ],
        [
         "7648",
         "0"
        ],
        [
         "4217",
         "1"
        ],
        [
         "10474",
         "0"
        ],
        [
         "1185",
         "1"
        ],
        [
         "46701",
         "1"
        ],
        [
         "26826",
         "1"
        ],
        [
         "12090",
         "1"
        ],
        [
         "5386",
         "1"
        ],
        [
         "17351",
         "0"
        ],
        [
         "18844",
         "1"
        ],
        [
         "28816",
         "1"
        ],
        [
         "43052",
         "0"
        ],
        [
         "11347",
         "1"
        ],
        [
         "42802",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 40000
       }
      },
      "text/plain": [
       "39087    0\n",
       "30893    0\n",
       "45278    1\n",
       "16398    0\n",
       "13653    0\n",
       "        ..\n",
       "11284    1\n",
       "44732    1\n",
       "38158    0\n",
       "860      1\n",
       "15795    1\n",
       "Name: sentiment, Length: 40000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OzLguIWdeB-5",
    "nbgrader": {
     "checksum": "0b30a67f0a9a62fae89dd8d046c70438",
     "grade": false,
     "grade_id": "cell-8b95ac774030229c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "Inspect the frequency of each sentiment in the training dataset (it is balanced!)\n",
    "\n",
    "This code calculates the relative frequency of each sentiment class in the training dataset. The `value_counts()` method counts the occurrences of each unique value in `y_train`, which represents the sentiment labels. By dividing these counts by the total number of samples (`y_train.shape[0]`), the code computes the proportion of each class within the training set. Assigning the result to `frequency` provides a quick way to check if the dataset is balanced or if one class is more prevalent than the other. Displaying `frequency` helps you understand the class distribution, which is important for evaluating and improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "1dVkRgPIuneP",
    "nbgrader": {
     "checksum": "256c996847f61b8dc7c6e046d1198a9c",
     "grade": false,
     "grade_id": "cell-e3bd86db55a92c1b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "71478b47-0750-4dc1-ec2e-9fa8a296fad0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6d0fbbfc-312e-4e71-a2e9-ec97ab91f0bf",
       "rows": [
        [
         "0",
         "0.500975"
        ],
        [
         "1",
         "0.499025"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "sentiment\n",
       "0    0.500975\n",
       "1    0.499025\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 5\n",
    "# Calculate the training data's frequency and assign the output to \"frequency\"\n",
    "frequency = y_train.value_counts() / y_train.shape[0]\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code converts the target variables `y_train` and `y_test` from integer labels to one-hot encoded arrays. The `pd.get_dummies()` function creates dummy variables for each class, turning each label into a binary vector where only the index corresponding to the class is set to 1 and all others are 0. The `.to_numpy()` method then converts the resulting DataFrame into a NumPy array, which is the format required by many machine learning models, especially neural networks. This transformation is essential for multi-class classification tasks, as it allows the model to output probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "id": "sia5LebDeuDM",
    "nbgrader": {
     "checksum": "3696f28f069bdf94d5ca55a10f13b6f5",
     "grade": false,
     "grade_id": "cell-d9c0261312c36fad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# question 6\n",
    "# Let's turn the target into a dummy vector\n",
    "y_train = pd.get_dummies(y_train).to_numpy()\n",
    "y_test  = pd.get_dummies(y_test).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "The code `y_train.shape` returns the dimensions of the `y_train` array. In this context, after converting `y_train` to a one-hot encoded NumPy array, `y_train.shape` will output a tuple indicating the number of samples and the number of classes. This is useful for verifying that the target variable has been correctly transformed and matches the expected input shape for machine learning models, especially neural networks that require specific input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d6cc03e20ea5c8a9ed9e4dd65cb22c9",
     "grade": false,
     "grade_id": "cell-12af49dffe46d4f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "p4ktfnOWeX-x",
    "nbgrader": {
     "checksum": "53482d2ab677e371636a3b13c495a694",
     "grade": false,
     "grade_id": "cell-8fdd5bd24c5b1aa8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "Unigram Multi-hot Encoding Baseline\n",
    "\n",
    "Next, let us see the performance of a neural net that is trained from the scratch using multi-hot encoding. \n",
    "\n",
    "This code sets up text preprocessing for a neural network model. The variable `max_tokens` is assigned the value 2412, which defines the maximum vocabulary size—the largest number of unique words the model will consider from the dataset. The `TextVectorization` layer from Keras is then initialized with this vocabulary limit and configured to use `multi_hot` encoding. With `multi_hot` encoding, each input text is converted into a binary vector of length `max_tokens`, where each position indicates whether a specific word from the vocabulary appears in the text (1) or not (0). This representation is useful for feeding text data into machine learning models, as it transforms raw text into a fixed-size, numeric format that captures word presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "id": "h48xnYwyXLWK",
    "nbgrader": {
     "checksum": "7f66cef5156075bb4fe7dc9d70a97df0",
     "grade": false,
     "grade_id": "cell-e9277fc1e87a127a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the maximum number of tokens to 2412. \n",
    "# Also set up our Text Vectorization layer using multi-hot encoding\n",
    "max_tokens      = 2412\n",
    "text_vectorization = TextVectorization(max_tokens  = max_tokens, \n",
    "                                       output_mode = 'multi_hot') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "The code `text_vectorization.adapt(X_train)` prepares the `TextVectorization` layer by analyzing the training data. This step builds the vocabulary from the text samples in `X_train`, allowing the layer to learn which words are present and how to map them to indices in the output vectors. Adapting on the training set ensures that the vocabulary reflects the data the model will learn from, which helps prevent information leakage from the test set and improves generalization. This is a crucial preprocessing step before transforming text data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1cfb54eadaf4db959ada980f108ab13a",
     "grade": false,
     "grade_id": "cell-d866880efca5c5ba",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# The vocabulary that will be indexed is given by the text corpus on our train dataset\n",
    "text_vectorization.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code applies the `TextVectorization` layer to both the training and testing datasets. By calling `text_vectorization(X_train)` and `text_vectorization(X_test)`, each text sample in `X_train` and `X_test` is transformed into a multi-hot encoded vector, where each element indicates the presence or absence of a specific word from the vocabulary. This step converts raw text data into a fixed-size, numeric format suitable for input into machine learning models, such as neural networks. Applying the same transformation to both sets ensures consistency in how the data is represented during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "id": "jO96Ho9WXVtb",
    "nbgrader": {
     "checksum": "c6cf40ac2eac47ef2c41e0a33e7046c6",
     "grade": false,
     "grade_id": "cell-8173b8e7c6ffe8a9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Question 7\n",
    "# We vectorize our input\n",
    "X_train = text_vectorization(X_train)\n",
    "X_test  = text_vectorization(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code defines a simple neural network model using Keras for text classification. The first line creates an input layer that expects vectors of length `max_tokens`, matching the size of the multi-hot encoded text data. The next line adds a dense (fully connected) layer with 32 units and ReLU activation, which helps the model learn non-linear relationships in the data. A dropout layer with a rate of 0.5 follows, randomly setting half of the input units to zero during training to help prevent overfitting. The output layer is another dense layer with 2 units and softmax activation, producing probabilities for each of the two sentiment classes (positive or negative). The model is then constructed by specifying the input and output layers, and `model.summary()` displays a summary of the model architecture, including the layers and the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "366225b8fe209f21675f420d258e870a",
     "grade": false,
     "grade_id": "cell-d039bb02c0b80a1a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2412</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">77,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2412\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m77,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,282</span> (301.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m77,282\u001b[0m (301.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,282</span> (301.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m77,282\u001b[0m (301.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 8\n",
    "# Now create your model. start with 32 dense relu layers, a dropout layer of 0.5, and a final softmax layer\n",
    "inputs  = keras.Input(shape=(max_tokens, ))\n",
    "x       = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x       = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "model   = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code compiles the Keras model, specifying how it should be trained. The `optimizer='adam'` argument selects the Adam optimization algorithm, which is widely used for its efficiency and adaptive learning rate. The `loss='categorical_crossentropy'` argument sets the loss function to categorical cross-entropy, which is appropriate for multi-class classification problems with one-hot encoded targets. The `metrics=['accuracy']` argument tells Keras to track accuracy during training and evaluation, providing a straightforward measure of model performance. Compiling the model with these settings prepares it for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "07007d98c76cf4987306051fa690778a",
     "grade": false,
     "grade_id": "cell-bf0c9b489f54a4f1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile your model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code trains the neural network model using the `fit` method from Keras. The training data (`X_train` and `y_train`) is provided, with the target labels converted to `float32` for compatibility with the model. The `validation_data` argument supplies the test set (`X_test` and `y_test`, also as `float32`), allowing the model to evaluate its performance on unseen data after each epoch. The `epochs=5` parameter specifies that the model will train for five complete passes through the training dataset. This setup helps monitor both training and validation accuracy, making it easier to detect overfitting or underfitting during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "95b5575361496edbb11f0c060a42679a",
     "grade": false,
     "grade_id": "cell-dd0a166873b6f420",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - accuracy: 0.7822 - loss: 0.4466 - val_accuracy: 0.8791 - val_loss: 0.2823\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570us/step - accuracy: 0.8799 - loss: 0.2928 - val_accuracy: 0.8816 - val_loss: 0.2805\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 556us/step - accuracy: 0.8928 - loss: 0.2648 - val_accuracy: 0.8790 - val_loss: 0.2870\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659us/step - accuracy: 0.9014 - loss: 0.2467 - val_accuracy: 0.8805 - val_loss: 0.2938\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700us/step - accuracy: 0.9088 - loss: 0.2280 - val_accuracy: 0.8780 - val_loss: 0.2970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31d9b50d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "# Use one-hot encoded y for training and testing\n",
    "model.fit(\n",
    "    x              = X_train, \n",
    "    y              = y_train.astype('float32'), \n",
    "    validation_data= (X_test, \n",
    "                      y_test.astype('float32')),\n",
    "    epochs         = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code evaluates the trained model's performance on the test dataset. The `model.evaluate` function computes the loss and accuracy using the test features (`X_test`) and one-hot encoded test labels (`y_test`), which are converted to `float32` for compatibility. The result is a list where the second element (`[1]`) represents the accuracy. The expression checks if this accuracy is greater than 0.85, returning `True` if the model achieves at least 85% accuracy on the test set. This is a quick way to verify if the model meets a desired performance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d38d17e24c54522e921010e1dabfe031",
     "grade": false,
     "grade_id": "cell-19919983da9eb5f0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step - accuracy: 0.8788 - loss: 0.2946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 9\n",
    "# Evaluate your model. You should be able to get your model to 85% at this point\n",
    "# Use one-hot encoded y for evaluation as well\n",
    "model.evaluate(x=X_test, y=y_test.astype('float32'))[1] > 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hlSWRoTGqn7Y",
    "nbgrader": {
     "checksum": "f24877faf0501bb073b82d4907dc36b9",
     "grade": false,
     "grade_id": "cell-70a2245e1fd4ffbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "Extend Baseline Model\n",
    "\n",
    "Let's create more complex models to increase the accuracy on our test sample. Try combining different models by changing:\n",
    "- Number of hidden units\n",
    "- Adding another hidden layer.\n",
    "- Changing the number of epochs.\n",
    "- Using bigrams instead of unigrams.\n",
    "\n",
    "To guide your search for the best parameters, note how the accuracy changes on both train and test data.\n",
    "\n",
    "This code sets up a neural network model for text classification using TensorFlow's Keras API. The variables `embedding_dim` and `hidden_units` are defined for potential use in model configuration, though only `hidden_units` is referenced in the dense layer. `num_classes` is determined by the number of columns in `y_train`, which is one-hot encoded, representing the number of output classes.\n",
    "\n",
    "The model is built using the `Sequential` API. It starts with an input layer that expects vectors of length `max_tokens`, matching the size of the multi-hot encoded text data. The first hidden layer is a dense (fully connected) layer with 128 units and ReLU activation, which helps the model learn complex patterns in the data. A dropout layer with a rate of 0.5 follows, randomly dropping half of the units during training to reduce overfitting. The output layer is a dense layer with `num_classes` units and softmax activation, producing a probability distribution over all possible classes. This architecture is suitable for multi-class classification tasks and is commonly used as a baseline for text classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "72tQJQOWqrxG",
    "nbgrader": {
     "checksum": "bc6d313f23848cca133da79089f248fa",
     "grade": false,
     "grade_id": "cell-d123d80f6d95e712",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_units  = 32\n",
    "num_classes   = y_train.shape[1]  # Use y_train, which is one-hot encoded\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(max_tokens,)),  # Use max_tokens as input shape\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: cyan;\">\n",
    "\n",
    "This code demonstrates how to set up and train a deep learning model using TensorFlow and Keras on the CIFAR-100 image dataset. First, the CIFAR-100 dataset is loaded, splitting the data into training and testing sets. The model is defined using the ResNet50 architecture, a popular convolutional neural network for image classification. The model is initialized without pre-trained weights (`weights=None`), and the input shape is set to match CIFAR-100 images (32x32 pixels with 3 color channels). The number of output classes is set to 100, corresponding to the dataset's categories.\n",
    "\n",
    "The loss function is specified as sparse categorical cross-entropy, suitable for integer class labels. The model is compiled with the Adam optimizer and configured to track accuracy during training. Finally, the model is trained for 5 epochs with a batch size of 64, using the training data. This process allows the model to learn to classify images into one of 100 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 326ms/step - accuracy: 0.0498 - loss: 5.0023\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 328ms/step - accuracy: 0.0758 - loss: 4.6368\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 325ms/step - accuracy: 0.1159 - loss: 4.1135\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 327ms/step - accuracy: 0.1780 - loss: 3.6378\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 325ms/step - accuracy: 0.1901 - loss: 3.6509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x32e3490d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top = True,\n",
    "    weights     = None,\n",
    "    input_shape = (32, 32, 3),\n",
    "    classes     = 100,)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0401 - loss: 2115.0334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test, y=y_test.astype('float32'))[1] > 0.00"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
